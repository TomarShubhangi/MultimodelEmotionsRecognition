{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgNk4N0k8mns",
        "outputId": "7ff3de45-a884-4c2e-9e09-b2f3959b0ec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxTNr-kc65Tj",
        "outputId": "6729d051-3be3-4e4f-dfbd-250ea5400b1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.13)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0ZuxZUP7PL1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "import gradio as gr\n",
        "import gradio as gr\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing import image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2HeagAh7rDu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import cv2\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCi1txzw7VER",
        "outputId": "75e1ebb3-b386-4716-dee5-9c37661568ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "model = load_model('/content/drive/MyDrive/facial_emotion_detection_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ5v6Zlx9Lt5",
        "outputId": "0fc7239e-99f2-4f74-eaac-9776018ce8c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'angry',\n",
              " 1: 'disgust',\n",
              " 2: 'fear',\n",
              " 3: 'happy',\n",
              " 4: 'neutral',\n",
              " 5: 'sad',\n",
              " 6: 'surprise'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Emotion labels dictionary\n",
        "emotion_labels = {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n",
        "index_to_emotion = {v: k for k, v in emotion_labels.items()}\n",
        "index_to_emotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "ePCAHlwG9M0u",
        "outputId": "d8accdbc-485f-4f01-df25-d1801ddead25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e82350ba07bda3bb77.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://e82350ba07bda3bb77.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def prepare_image(img_pil):\n",
        "    \"\"\"Preprocess the PIL image to fit your model's input requirements.\"\"\"\n",
        "    # Convert the PIL image to a numpy array with the target size\n",
        "    img = img_pil.resize((224, 224))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Convert single image to a batch.\n",
        "    img_array /= 255.0  # Rescale pixel values to [0,1], as done during training\n",
        "    return img_array\n",
        "\n",
        "\n",
        "\n",
        "# Define the Gradio interface\n",
        "def predict_emotion(image):\n",
        "    # Preprocess the image\n",
        "    processed_image = prepare_image(image)\n",
        "    # Make prediction using the model\n",
        "    prediction = model.predict(processed_image)\n",
        "    # Get the emotion label with the highest probability\n",
        "    predicted_class = np.argmax(prediction, axis=1)\n",
        "    predicted_emotion = index_to_emotion.get(predicted_class[0], \"Unknown Emotion\")\n",
        "    return predicted_emotion\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=predict_emotion,  # Your prediction function\n",
        "    inputs=gr.Image(type=\"pil\"),  # Input for uploading an image, directly compatible with PIL images\n",
        "    outputs=\"text\",  # Output as text displaying the predicted emotion\n",
        "    title=\"Emotion Detection\",\n",
        "    description=\"Upload an image and see the predicted emotion.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "interface.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7j6tE-GE9i3j"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image):\n",
        "    # Resize and normalize as per training\n",
        "    img = cv2.resize(image, (224, 224))  # assuming input shape was 224x224\n",
        "    img = img / 255.0  # normalize\n",
        "    img = np.expand_dims(img, axis=0)  # add batch dimension\n",
        "    return img\n",
        "\n",
        "def predict_emotion(image):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "    img = preprocess_image(image)\n",
        "    preds = model.predict(img)\n",
        "    emotion = emotion_labels[np.argmax(preds)]\n",
        "    confidence = np.max(preds)\n",
        "    return f\"{emotion} ({confidence*100:.2f}%)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "JZWxaYY79nz_",
        "outputId": "011ecce2-e46e-4806-b618-909b11b47d54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a49bcf4c3f236e31b7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://a49bcf4c3f236e31b7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "iface = gr.Interface(\n",
        "    fn=predict_emotion,\n",
        "    inputs=gr.Image(type=\"numpy\", label=\"Upload Face Image\"),\n",
        "    outputs=gr.Textbox(label=\"Predicted Emotion\"),\n",
        "    title=\"Facial Emotion Detection (ResNet Model)\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "zuGQAETf-a-5",
        "outputId": "b442d843-75b8-4b81-b518-73ab706538d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://2ad4b4c7957e8c6bc2.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://2ad4b4c7957e8c6bc2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from PIL import Image\n",
        "\n",
        "# Load your model\n",
        "model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")  # Change to your actual model path\n",
        "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']  # Update if needed\n",
        "\n",
        "# Ensure correct input shape (likely 4096, check your model summary)\n",
        "def prepare_image(img_pil):\n",
        "    img = img_pil.convert(\"L\")             # Convert to grayscale\n",
        "    img = img.resize((64, 64))             # Try 64x64 if model expects 4096: 64x64 = 4096\n",
        "    img_array = img_to_array(img) / 255.0  # Normalize\n",
        "    img_array = img_array.reshape(1, -1)   # Flatten to (1, 4096)\n",
        "    return img_array\n",
        "\n",
        "# Predict function\n",
        "def detect_emotion(image):\n",
        "    try:\n",
        "        processed_image = prepare_image(image)\n",
        "        prediction = model.predict(processed_image)\n",
        "        predicted_index = np.argmax(prediction)\n",
        "        predicted_class = class_names[predicted_index]\n",
        "        confidence = round(prediction[0][predicted_index] * 100, 2)\n",
        "        return f\"{predicted_class} ({confidence}%)\"\n",
        "    except Exception as e:\n",
        "        return f\"Prediction Error: {str(e)}\"\n",
        "\n",
        "# Gradio UI\n",
        "iface = gr.Interface(\n",
        "    fn=detect_emotion,\n",
        "    inputs=gr.Image(type=\"pil\", label=\"Upload a face image to detect emotion.\"),\n",
        "    outputs=gr.Textbox(label=\"Predicted Emotion\"),\n",
        "    title=\"Facial Emotion Detection\",\n",
        "    description=\"Upload a face image and the model will predict the emotion.\",\n",
        "    theme=\"default\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6fq-hW6dAMD",
        "outputId": "4f497fa7-0db0-4a6a-da0d-bf415582c9f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, 48, 48, 1)\n"
          ]
        }
      ],
      "source": [
        "print(model.input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "EZhZpbR_b9Tq",
        "outputId": "e0872dbb-1e7e-4f87-a192-682632bf679f"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gradio'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1df71b19c374>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgradio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradio'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from PIL import Image\n",
        "\n",
        "# Load the trained model and class names\n",
        "model = load_model(\"your_model_path.h5\")  # Replace with your actual model path\n",
        "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']  # Update if different\n",
        "\n",
        "# Function to preprocess the image for prediction\n",
        "def prepare_image(img_pil):\n",
        "    img = img_pil.convert(\"L\")  # Convert to grayscale\n",
        "    img = img.resize((48, 48))  # Resize to match training input\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    img_array = img_array.reshape(1, -1)  # Flatten to shape (1, 2304)\n",
        "    return img_array\n",
        "\n",
        "# Function to predict emotion\n",
        "def detect_emotion(image):\n",
        "    try:\n",
        "        processed_image = prepare_image(image)\n",
        "        prediction = model.predict(processed_image)\n",
        "        predicted_index = np.argmax(prediction)\n",
        "        predicted_class = class_names[predicted_index]\n",
        "        confidence = round(prediction[0][predicted_index] * 100, 2)\n",
        "        return f\"{predicted_class} ({confidence}%)\"\n",
        "    except Exception as e:\n",
        "        return f\"Prediction Error: {str(e)}\"\n",
        "\n",
        "# Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=detect_emotion,\n",
        "    inputs=gr.Image(type=\"pil\", label=\"Upload a face image to detect emotion.\"),\n",
        "    outputs=gr.Textbox(label=\"Predicted Emotion\"),\n",
        "    title=\"Facial Emotion Detection\",\n",
        "    description=\"This app detects emotion from facial expressions using a pre-trained model.\",\n",
        "    theme=\"default\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "NtXxTeaydSLl",
        "outputId": "0e74aae2-06e9-45cd-b7d3-cb672f0de5b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d890dd4122cd7896ef.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d890dd4122cd7896ef.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from PIL import Image\n",
        "\n",
        "# Load model and class names\n",
        "model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")  # replace with your actual model path\n",
        "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']  # use your actual labels\n",
        "\n",
        "# Image preprocessing\n",
        "def prepare_image(img_pil):\n",
        "    img = img_pil.convert(\"L\")                  # convert to grayscale\n",
        "    img = img.resize((48, 48))                  # resize to match model input\n",
        "    img_array = img_to_array(img) / 255.0       # normalize\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # shape becomes (1, 48, 48, 1)\n",
        "    return img_array\n",
        "\n",
        "# Emotion detection\n",
        "def detect_emotion(image):\n",
        "    try:\n",
        "        processed_image = prepare_image(image)\n",
        "        prediction = model.predict(processed_image)\n",
        "        predicted_index = np.argmax(prediction)\n",
        "        predicted_class = class_names[predicted_index]\n",
        "        confidence = round(float(prediction[0][predicted_index]) * 100, 2)\n",
        "        return f\"{predicted_class} ({confidence}%)\"\n",
        "    except Exception as e:\n",
        "        return f\"Prediction Error: {str(e)}\"\n",
        "\n",
        "# Gradio Interface\n",
        "iface = gr.Interface(\n",
        "    fn=detect_emotion,\n",
        "    inputs=gr.Image(type=\"pil\", label=\"Upload a face image to detect emotion.\"),\n",
        "    outputs=gr.Textbox(label=\"Predicted Emotion\"),\n",
        "    title=\"Facial Emotion Detection\",\n",
        "    description=\"Upload a face image and the model will predict the emotion.\",\n",
        "    theme=\"default\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "ieeRVcktfSEt",
        "outputId": "334d3de4-3ce4-4fb7-9ccf-64605e86b402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://15e0a471b589dc5cba.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://15e0a471b589dc5cba.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# Load text model components\n",
        "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
        "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
        "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
        "\n",
        "# Load facial model\n",
        "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
        "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "\n",
        "# ---------- TEXT EMOTION FUNCTIONS ----------\n",
        "def clean_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "    text = text.lower().split()\n",
        "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
        "    return \" \".join(text)\n",
        "\n",
        "def predict_emotion(input_text):\n",
        "    cleaned_text = clean_text(input_text)\n",
        "    input_vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
        "    predicted_label = lg.predict(input_vectorized)[0]\n",
        "    predicted_emotion = lb.inverse_transform([predicted_label])[0]\n",
        "    probability = np.max(lg.predict_proba(input_vectorized))\n",
        "    return f\"Emotion: {predicted_emotion}, Probability: {probability:.2f}\"\n",
        "\n",
        "\n",
        "# ---------- FACIAL EMOTION FUNCTIONS ----------\n",
        "def prepare_image(img_pil):\n",
        "    img = img_pil.convert(\"L\")\n",
        "    img = img.resize((48, 48))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return img_array\n",
        "\n",
        "def detect_emotion(image):\n",
        "    try:\n",
        "        processed_image = prepare_image(image)\n",
        "        prediction = facial_model.predict(processed_image)\n",
        "        predicted_index = np.argmax(prediction)\n",
        "        predicted_class = class_names[predicted_index]\n",
        "        confidence = round(float(prediction[0][predicted_index]) * 100, 2)\n",
        "        return f\"{predicted_class} ({confidence}%)\"\n",
        "    except Exception as e:\n",
        "        return f\"Prediction Error: {str(e)}\"\n",
        "\n",
        "\n",
        "# ---------- GRADIO UI ----------\n",
        "with gr.Blocks(title=\"Emotion Detection Suite\") as app:\n",
        "    gr.Markdown(\n",
        "        \"<h1 style='text-align: center; color: #3B82F6;'>Multimodal Emotion Detection App</h1>\"\n",
        "        \"<p style='text-align: center;'>Detect emotions from text and facial expressions</p>\"\n",
        "    )\n",
        "\n",
        "    with gr.Tab(\"Text Emotion Detection\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                text_input = gr.Textbox(lines=3, placeholder=\"Type a sentence...\", label=\"Enter your text\")\n",
        "                text_examples = gr.Examples(\n",
        "                    examples=[\n",
        "                        \"I am feeling fantastic today!\",\n",
        "                        \"Why does everything feel so stressful?\",\n",
        "                        \"I'm scared about tomorrow's interview.\",\n",
        "                        \"This makes me so angry!\",\n",
        "                        \"What a pleasant surprise!\",\n",
        "                        \"I‚Äôm just feeling neutral right now.\"\n",
        "                    ],\n",
        "                    inputs=text_input\n",
        "                )\n",
        "                text_button = gr.Button(\"Analyze Emotion\")\n",
        "            with gr.Column():\n",
        "                text_output = gr.Textbox(label=\"Predicted Emotion\")\n",
        "\n",
        "        text_button.click(fn=predict_emotion, inputs=text_input, outputs=text_output)\n",
        "\n",
        "    with gr.Tab(\"Facial Emotion Detection\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                image_input = gr.Image(type=\"pil\", label=\"Upload a face image\")\n",
        "                image_button = gr.Button(\"Detect Emotion\")\n",
        "            with gr.Column():\n",
        "                image_output = gr.Textbox(label=\"Predicted Emotion\")\n",
        "\n",
        "        image_button.click(fn=detect_emotion, inputs=image_input, outputs=image_output)\n",
        "\n",
        "app.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wkt4NzI4PY-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# Load text model components\n",
        "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
        "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
        "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
        "\n",
        "# Load facial model\n",
        "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
        "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "# ---------- TEXT EMOTION FUNCTIONS ----------\n",
        "def clean_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "    text = text.lower().split()\n",
        "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
        "    return \" \".join(text)\n",
        "\n",
        "def predict_emotion(input_text):\n",
        "    cleaned_text = clean_text(input_text)\n",
        "    input_vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
        "    predicted_label = lg.predict(input_vectorized)[0]\n",
        "    predicted_emotion = lb.inverse_transform([predicted_label])[0]\n",
        "    probability = np.max(lg.predict_proba(input_vectorized))\n",
        "    return f\"Emotion: {predicted_emotion}, Probability: {probability:.2f}\"\n",
        "\n",
        "# ---------- FACIAL EMOTION FUNCTIONS ----------\n",
        "def prepare_image(img_pil):\n",
        "    img = img_pil.convert(\"L\")\n",
        "    img = img.resize((48, 48))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return img_array\n",
        "\n",
        "def detect_emotion(image):\n",
        "    try:\n",
        "        processed_image = prepare_image(image)\n",
        "        prediction = facial_model.predict(processed_image)\n",
        "        predicted_index = np.argmax(prediction)\n",
        "        predicted_class = class_names[predicted_index]\n",
        "        confidence = round(float(prediction[0][predicted_index]) * 100, 2)\n",
        "        return f\"{predicted_class} ({confidence}%)\"\n",
        "    except Exception as e:\n",
        "        return f\"Prediction Error: {str(e)}\"\n",
        "\n",
        "# ---------- FUSION FUNCTION ----------\n",
        "def fusion_emotion_analysis(text, image):\n",
        "    result = \"\"\n",
        "    text_emotion = \"N/A\"\n",
        "    text_conf = 0.0\n",
        "    face_emotion = \"N/A\"\n",
        "    face_conf = 0.0\n",
        "\n",
        "    # Text Prediction\n",
        "    if text:\n",
        "        cleaned_text = clean_text(text)\n",
        "        input_vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
        "        predicted_text_label = lg.predict(input_vectorized)[0]\n",
        "        text_emotion = lb.inverse_transform([predicted_text_label])[0]\n",
        "        text_conf = np.max(lg.predict_proba(input_vectorized))\n",
        "\n",
        "    # Face Prediction\n",
        "    if image:\n",
        "        try:\n",
        "            processed_image = prepare_image(image)\n",
        "            prediction = facial_model.predict(processed_image)\n",
        "            face_index = np.argmax(prediction)\n",
        "            face_emotion = class_names[face_index]\n",
        "            face_conf = float(prediction[0][face_index])\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Fusion Logic\n",
        "    if face_conf > text_conf:\n",
        "        dominant = face_emotion\n",
        "    else:\n",
        "        dominant = text_emotion\n",
        "\n",
        "    result += f\"üìù Text Emotion: {text_emotion} ({text_conf:.2f})\\n\"\n",
        "    result += f\"üì∑ Face Emotion: {face_emotion} ({face_conf:.2f})\\n\"\n",
        "    result += f\"\\nüí° Final Dominant Emotion: **{dominant}**\"\n",
        "\n",
        "    return result\n",
        "\n",
        "# ---------- GRADIO UI ----------\n",
        "with gr.Blocks(title=\"Emotion Detection Suite\") as app:\n",
        "    gr.Markdown(\n",
        "        \"<h1 style='text-align: center; color: #3B82F6;'>Multimodal Emotion Detection App</h1>\"\n",
        "        \"<p style='text-align: center;'>Detect emotions from text and facial expressions</p>\"\n",
        "    )\n",
        "\n",
        "    with gr.Tab(\"Text Emotion Detection\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                text_input = gr.Textbox(lines=3, placeholder=\"Type a sentence...\", label=\"Enter your text\")\n",
        "                text_examples = gr.Examples(\n",
        "                    examples=[\n",
        "                        \"I am feeling fantastic today!\",\n",
        "                        \"Why does everything feel so stressful?\",\n",
        "                        \"I'm scared about tomorrow's interview.\",\n",
        "                        \"This makes me so angry!\",\n",
        "                        \"What a pleasant surprise!\",\n",
        "                        \"I‚Äôm just feeling neutral right now.\"\n",
        "                    ],\n",
        "                    inputs=text_input\n",
        "                )\n",
        "                text_button = gr.Button(\"Analyze Emotion\")\n",
        "            with gr.Column():\n",
        "                text_output = gr.Textbox(label=\"Predicted Emotion\")\n",
        "\n",
        "        text_button.click(fn=predict_emotion, inputs=text_input, outputs=text_output)\n",
        "\n",
        "    with gr.Tab(\"Facial Emotion Detection\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                image_input = gr.Image(type=\"pil\", label=\"Upload a face image\")\n",
        "                image_button = gr.Button(\"Detect Emotion\")\n",
        "            with gr.Column():\n",
        "                image_output = gr.Textbox(label=\"Predicted Emotion\")\n",
        "\n",
        "        image_button.click(fn=detect_emotion, inputs=image_input, outputs=image_output)\n",
        "\n",
        "    with gr.Tab(\"üß† Fusion: Text + Face\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                fusion_text = gr.Textbox(label=\"Enter Text\", lines=2, placeholder=\"Type something...\")\n",
        "                fusion_image = gr.Image(type=\"pil\", label=\"Upload a face image\")\n",
        "                fusion_button = gr.Button(\"Analyze Both\")\n",
        "            with gr.Column():\n",
        "                fusion_output = gr.Textbox(label=\"Fusion Result\", lines=6)\n",
        "\n",
        "        fusion_button.click(fn=fusion_emotion_analysis, inputs=[fusion_text, fusion_image], outputs=fusion_output)\n",
        "\n",
        "app.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "v2jV3gGFPYbM",
        "outputId": "4a801d10-1183-4107-dc61-48410d41ea98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://96a07b6747d879eaa8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://96a07b6747d879eaa8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oSOcrEq1PHOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
        "import torch\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "# ------------------- TEXT EMOTION -------------------\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
        "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
        "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
        "\n",
        "def clean_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "    text = text.lower().split()\n",
        "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
        "    return \" \".join(text)\n",
        "\n",
        "def predict_text_emotion(input_text):\n",
        "    cleaned = clean_text(input_text)\n",
        "    vec = tfidf_vectorizer.transform([cleaned])\n",
        "    label = lg.predict(vec)[0]\n",
        "    emotion = lb.inverse_transform([label])[0]\n",
        "    prob = np.max(lg.predict_proba(vec))\n",
        "    return f\"Emotion: {emotion}, Probability: {prob:.2f}\"\n",
        "\n",
        "# ------------------- FACIAL EMOTION -------------------\n",
        "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
        "face_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "def prepare_image(img):\n",
        "    img = img.convert(\"L\").resize((48, 48))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    return np.expand_dims(img_array, axis=0)\n",
        "\n",
        "def predict_face_emotion(image):\n",
        "    try:\n",
        "        processed = prepare_image(image)\n",
        "        preds = facial_model.predict(processed)\n",
        "        idx = np.argmax(preds)\n",
        "        conf = round(float(preds[0][idx]) * 100, 2)\n",
        "        return f\"{face_labels[idx]} ({conf}%)\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# ------------------- SPEECH EMOTION -------------------\n",
        "hf_model_name = \"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\"\n",
        "processor = Wav2Vec2Processor.from_pretrained(hf_model_name)\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(hf_model_name)\n",
        "label_mapping = model.config.id2label\n",
        "\n",
        "def classify_speech(audio_path):\n",
        "    try:\n",
        "        speech, sr = librosa.load(audio_path, sr=16000)\n",
        "        inputs = processor(speech, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "        with torch.no_grad():\n",
        "            logits = model(**inputs).logits\n",
        "        predicted_id = torch.argmax(logits, dim=-1).item()\n",
        "        predicted_label = label_mapping[predicted_id]\n",
        "        confidence = torch.softmax(logits, dim=-1)[0][predicted_id].item()\n",
        "        return f\"Emotion: {predicted_label}, Confidence: {confidence:.2f}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# ------------------- GRADIO UI -------------------\n",
        "with gr.Blocks(title=\"Multimodal Emotion Detection\") as app:\n",
        "    gr.Markdown(\"\"\"\n",
        "    <h1 style='text-align: center; color: #3B82F6;'>Multimodal Emotion Detection</h1>\n",
        "    <p style='text-align: center;'>Detect emotions from text, facial images, and speech</p>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Tab(\"Text Emotion\"):\n",
        "        txt_input = gr.Textbox(lines=2, label=\"Enter text\")\n",
        "        txt_btn = gr.Button(\"Analyze Text Emotion\")\n",
        "        txt_out = gr.Textbox(label=\"Detected Emotion\")\n",
        "        txt_btn.click(fn=predict_text_emotion, inputs=txt_input, outputs=txt_out)\n",
        "\n",
        "    with gr.Tab(\"Facial Emotion\"):\n",
        "        img_input = gr.Image(type=\"pil\", label=\"Upload Face Image\")\n",
        "        img_btn = gr.Button(\"Detect Facial Emotion\")\n",
        "        img_out = gr.Textbox(label=\"Detected Emotion\")\n",
        "        img_btn.click(fn=predict_face_emotion, inputs=img_input, outputs=img_out)\n",
        "\n",
        "    with gr.Tab(\"Speech Emotion\"):\n",
        "        audio_input = gr.Audio(type=\"filepath\", label=\"Upload .wav file\")\n",
        "        audio_btn = gr.Button(\"Analyze Speech Emotion\")\n",
        "        audio_out = gr.Textbox(label=\"Detected Emotion\")\n",
        "        audio_btn.click(fn=classify_speech, inputs=audio_input, outputs=audio_out)\n",
        "\n",
        "app.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856,
          "referenced_widgets": [
            "70255892a7b24d668d58ed0c0c63c2f0",
            "2cef7670d08645d4ae5e968f7979bad7",
            "e3fd240723aa4328b8846577891cb69e",
            "ad8a48da34c24fb5abf1ac6e7e76928b",
            "987fce1bdad84aa8b441ab51123ca867",
            "c40711c0e1e548389bbca800035f095e",
            "fe682990628048649d28a58eeb05f5ef",
            "30325925153c4a5fae6afb53f9c5f4f0",
            "af34c0f5e44f4c339d22d0b7466fe2ba",
            "432e5436df3c4f5d929798ad95ff6262",
            "ad0cf33e9ab3432184ac2c53bc0d1213",
            "70fca7f28301467d860dc6cf4617ad23",
            "56736bb3ebf44eb38225f2b8e5a068fc",
            "d7cbcffd188844f393ebb7536dd021d2",
            "86b70bbda97940ca86706f38f4499c0b",
            "16cf59a900644f15a25ec3f31c793c5a",
            "ceb167c4927541b7908b4dd4929ca09f",
            "abc8b4e8f1eb41ecbf2168fd14e01301",
            "49689d00a654443ab5d355e757cf76b1",
            "4260b33d7e9843618950a3bace7f1038",
            "3ea8f5ba98e34b329895ee401281bb3a",
            "bb16d0d58f8641db8cc0bc99ad3fb858"
          ]
        },
        "id": "C0vD86LVRR68",
        "outputId": "96ccaaa3-e146-44e2-c027-2d60ca1ed032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70255892a7b24d668d58ed0c0c63c2f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70fca7f28301467d860dc6cf4617ad23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:312: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected str, bytes or os.PathLike object, not NoneType",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-3621743818.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# ------------------- SPEECH EMOTION -------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mhf_model_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWav2Vec2Processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhf_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWav2Vec2ForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhf_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mlabel_mapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             warnings.warn(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/processing_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m   1183\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_arguments_from_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m         \u001b[0mprocessor_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_processor_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_args_and_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/processing_utils.py\u001b[0m in \u001b[0;36m_get_arguments_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1246\u001b[0m                 \u001b[0mattribute_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_possibly_dynamic_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1248\u001b[0;31m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattribute_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1249\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m                     raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2023\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2025\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   2026\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2027\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2276\u001b[0m         \u001b[0;31m# Instantiate the tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2277\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2278\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2279\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mimport_protobuf_decode_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2280\u001b[0m             logger.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, bos_token, eos_token, unk_token, pad_token, word_delimiter_token, replace_word_delimiter_char, do_lower_case, target_lang, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_lang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvocab_handle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import torch\n",
        "import librosa\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "\n",
        "# ------------------- TEXT EMOTION -------------------\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
        "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
        "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
        "\n",
        "def clean_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "    text = text.lower().split()\n",
        "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
        "    return \" \".join(text)\n",
        "\n",
        "def predict_text_emotion(input_text):\n",
        "    cleaned = clean_text(input_text)\n",
        "    vec = tfidf_vectorizer.transform([cleaned])\n",
        "    label = lg.predict(vec)[0]\n",
        "    emotion = lb.inverse_transform([label])[0]\n",
        "    prob = np.max(lg.predict_proba(vec))\n",
        "    return f\"Emotion: {emotion}, Probability: {prob:.2f}\"\n",
        "\n",
        "# ------------------- FACIAL EMOTION -------------------\n",
        "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
        "face_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "def prepare_image(img):\n",
        "    img = img.convert(\"L\").resize((48, 48))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    return np.expand_dims(img_array, axis=0)\n",
        "\n",
        "def predict_face_emotion(image):\n",
        "    try:\n",
        "        processed = prepare_image(image)\n",
        "        preds = facial_model.predict(processed)\n",
        "        idx = np.argmax(preds)\n",
        "        conf = round(float(preds[0][idx]) * 100, 2)\n",
        "        return f\"{face_labels[idx]} ({conf}%)\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# ------------------- SPEECH EMOTION -------------------\n",
        "model_name = \"prithivMLmods/Speech-Emotion-Classification\"\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)\n",
        "processor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
        "\n",
        "id2label = {\n",
        "    0: \"Anger\", 1: \"Calm\", 2: \"Disgust\", 3: \"Fear\",\n",
        "    4: \"Happy\", 5: \"Neutral\", 6: \"Sad\", 7: \"Surprised\"\n",
        "}\n",
        "\n",
        "def classify_speech(audio_path):\n",
        "    try:\n",
        "        speech, sr = librosa.load(audio_path, sr=16000)\n",
        "        inputs = processor(speech, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
        "        with torch.no_grad():\n",
        "            logits = model(**inputs).logits\n",
        "            probs = torch.nn.functional.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
        "        top_idx = int(np.argmax(probs))\n",
        "        return {id2label[i]: float(probs[i]) for i in range(len(probs))}\n",
        "    except Exception as e:\n",
        "        return {\"Error\": str(e)}\n",
        "\n",
        "# ------------------- GRADIO UI -------------------\n",
        "with gr.Blocks(title=\"Multimodal Emotion Detection\") as app:\n",
        "    gr.Markdown(\"\"\"\n",
        "    <h1 style='text-align: center; color: #3B82F6;'>Multimodal Emotion Detection</h1>\n",
        "    <p style='text-align: center;'>Detect emotions from text, facial expressions, and speech</p>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Tab(\"Text Emotion\"):\n",
        "        txt_input = gr.Textbox(lines=2, label=\"Enter text\")\n",
        "        txt_btn = gr.Button(\"Analyze Text Emotion\")\n",
        "        txt_out = gr.Textbox(label=\"Detected Emotion\")\n",
        "        txt_btn.click(fn=predict_text_emotion, inputs=txt_input, outputs=txt_out)\n",
        "\n",
        "    with gr.Tab(\"Facial Emotion\"):\n",
        "        img_input = gr.Image(type=\"pil\", label=\"Upload Face Image\")\n",
        "        img_btn = gr.Button(\"Detect Facial Emotion\")\n",
        "        img_out = gr.Textbox(label=\"Detected Emotion\")\n",
        "        img_btn.click(fn=predict_face_emotion, inputs=img_input, outputs=img_out)\n",
        "\n",
        "    with gr.Tab(\"Speech Emotion\"):\n",
        "        audio_input = gr.Audio(type=\"filepath\", label=\"Upload .wav file\")\n",
        "        audio_btn = gr.Button(\"Analyze Speech Emotion\")\n",
        "        audio_out = gr.Label(num_top_classes=3, label=\"Detected Emotion & Confidence\")\n",
        "        audio_btn.click(fn=classify_speech, inputs=audio_input, outputs=audio_out)\n",
        "\n",
        "app.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794,
          "referenced_widgets": [
            "849c506eb9b04dbd8ada43b09f357fae",
            "9dd41c5bb4e3402d8902cafe107c2ce7",
            "89a2a334a2b44abbb10a736d994b4289",
            "509879ea62a84809aa05266c87c71a89",
            "aa71764071bf474099b5a749c746b5e7",
            "a02929f19d6545a9a0b44269f7218343",
            "42424cb0d84e48b1937a82df3ad8b422",
            "e1567e38a6b54460b70dd6bd0a214752",
            "3cd1d603be844446b65c549421357d27",
            "c3710531e49c46e6b5c5f4e46c6c4889",
            "92c52d4a7caa48a2a814dfd0c0a79b27",
            "772765d1a82c49ca91d898cc02c52222",
            "a56fcb59fc584bd4964d630a0ed0ca1e",
            "d1dcebd86ec34e3caae6ed8eb1b466a3",
            "166502e8de944d21ba4248042762790c",
            "779e86ea272c46c9822918b038403861",
            "c08fbefe13254c05929993186cd27135",
            "82c2dde1351c429ab86e09fe0de639e9",
            "60f9e63a0d3c43ca806a3cc1eee09d53",
            "488ed5e6c0f64180a5855288dce1bd79",
            "54395a0210204c8cbfa1e062807b9897",
            "8f4d4f201fa94d43bbb8099807a10b16",
            "96cfac83b0bc4043829e0762ba863816",
            "fa4293c74ab14fdaad192c5eea6e305a",
            "46bd1b82f91b4e4da93817fd1fb9b48e",
            "545a6d83c7484b72a4f7bb5c85a24a64",
            "8f1e49e7e0ba4d8f9fe47b735e31432f",
            "e1100dcfc56e41baa634cd50a69ece62",
            "62109e980a3b4f708c710669d897adcb",
            "d9bd330c063143638e3f54690df858b8",
            "ccab155a0c3846b18d1a34fdbe0db20a",
            "55da60e5b3c94fa1905589d070935d56",
            "b80c6fc6ffff4723a865139e433d9fc3"
          ]
        },
        "id": "K8g5lUpBR2Sq",
        "outputId": "49649aad-c2b8-44c8-e8ca-b437c6981b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "849c506eb9b04dbd8ada43b09f357fae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "772765d1a82c49ca91d898cc02c52222"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96cfac83b0bc4043829e0762ba863816"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ff6437e7c1aacef593.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ff6437e7c1aacef593.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rE_M67ZWSuJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import torch\n",
        "import librosa\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "\n",
        "# ------------------- TEXT EMOTION -------------------\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
        "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
        "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
        "\n",
        "def clean_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "    text = text.lower().split()\n",
        "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
        "    return \" \".join(text)\n",
        "\n",
        "def predict_text_emotion(input_text):\n",
        "    cleaned = clean_text(input_text)\n",
        "    vec = tfidf_vectorizer.transform([cleaned])\n",
        "    label = lg.predict(vec)[0]\n",
        "    emotion = lb.inverse_transform([label])[0]\n",
        "    prob = np.max(lg.predict_proba(vec))\n",
        "    return emotion, float(prob)\n",
        "\n",
        "# ------------------- FACIAL EMOTION -------------------\n",
        "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
        "face_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "def prepare_image(img):\n",
        "    img = img.convert(\"L\").resize((48, 48))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    return np.expand_dims(img_array, axis=0)\n",
        "\n",
        "def predict_face_emotion(image):\n",
        "    try:\n",
        "        processed = prepare_image(image)\n",
        "        preds = facial_model.predict(processed)\n",
        "        idx = np.argmax(preds)\n",
        "        conf = round(float(preds[0][idx]), 2)\n",
        "        return face_labels[idx], conf\n",
        "    except Exception as e:\n",
        "        return \"Error\", 0.0\n",
        "\n",
        "# ------------------- SPEECH EMOTION -------------------\n",
        "model_name = \"prithivMLmods/Speech-Emotion-Classification\"\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)\n",
        "processor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
        "\n",
        "id2label = {\n",
        "    0: \"Anger\", 1: \"Calm\", 2: \"Disgust\", 3: \"Fear\",\n",
        "    4: \"Happy\", 5: \"Neutral\", 6: \"Sad\", 7: \"Surprised\"\n",
        "}\n",
        "\n",
        "def classify_speech(audio_path):\n",
        "    try:\n",
        "        speech, sr = librosa.load(audio_path, sr=16000)\n",
        "        inputs = processor(speech, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
        "        with torch.no_grad():\n",
        "            logits = model(**inputs).logits\n",
        "            probs = torch.nn.functional.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
        "        top_idx = int(np.argmax(probs))\n",
        "        return id2label[top_idx], float(probs[top_idx])\n",
        "    except Exception as e:\n",
        "        return \"Error\", 0.0\n",
        "\n",
        "# ------------------- FUSION FUNCTION -------------------\n",
        "def fusion_model(text, image, audio):\n",
        "    text_emo, text_prob = predict_text_emotion(text) if text else (\"N/A\", 0)\n",
        "    face_emo, face_prob = predict_face_emotion(image) if image else (\"N/A\", 0)\n",
        "    speech_emo, speech_prob = classify_speech(audio) if audio else (\"N/A\", 0)\n",
        "\n",
        "    results = {\n",
        "        \"Text\": (text_emo, text_prob),\n",
        "        \"Facial\": (face_emo, face_prob),\n",
        "        \"Speech\": (speech_emo, speech_prob)\n",
        "    }\n",
        "    dominant = max(results.items(), key=lambda x: x[1][1])\n",
        "    fusion_summary = f\"\"\"\n",
        "Text Emotion: {text_emo} ({text_prob*100:.2f}%)\\n\n",
        "Facial Emotion: {face_emo} ({face_prob*100:.2f}%)\\n\n",
        "Speech Emotion: {speech_emo} ({speech_prob*100:.2f}%)\\n\n",
        "\n",
        "üëâ Dominant Emotion: **{dominant[0]} - {dominant[1][0]}**\n",
        "\"\"\"\n",
        "    return fusion_summary\n",
        "\n",
        "# ------------------- GRADIO UI -------------------\n",
        "with gr.Blocks(title=\"Multimodal Emotion Detection\") as app:\n",
        "    gr.Markdown(\"\"\"\n",
        "    <h1 style='text-align: center; color: #3B82F6;'>Multimodal Emotion Detection</h1>\n",
        "    <p style='text-align: center;'>Detect emotions from text, facial expressions, and speech</p>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Tab(\"Text Emotion\"):\n",
        "        txt_input = gr.Textbox(lines=2, label=\"Enter text\")\n",
        "        txt_examples = gr.Examples(\n",
        "            examples=[\n",
        "                \"I am feeling fantastic today!\",\n",
        "                \"Why does everything feel so stressful?\",\n",
        "                \"I'm scared about tomorrow's interview.\",\n",
        "                \"This makes me so angry!\",\n",
        "                \"What a pleasant surprise!\",\n",
        "                \"I‚Äôm just feeling neutral right now.\"\n",
        "            ],\n",
        "            inputs=txt_input\n",
        "        )\n",
        "        txt_btn = gr.Button(\"Analyze Text Emotion\")\n",
        "        txt_out = gr.Textbox(label=\"Detected Emotion\")\n",
        "\n",
        "        def wrap_text(text):\n",
        "            emotion, prob = predict_text_emotion(text)\n",
        "            return f\"Emotion: {emotion}, Probability: {prob:.2f}\"\n",
        "\n",
        "        txt_btn.click(fn=wrap_text, inputs=txt_input, outputs=txt_out)\n",
        "\n",
        "    with gr.Tab(\"Facial Emotion\"):\n",
        "        img_input = gr.Image(type=\"pil\", label=\"Upload Face Image\")\n",
        "        img_btn = gr.Button(\"Detect Facial Emotion\")\n",
        "        img_out = gr.Textbox(label=\"Detected Emotion\")\n",
        "\n",
        "        def wrap_face(img):\n",
        "            emo, conf = predict_face_emotion(img)\n",
        "            return f\"{emo} ({conf*100:.2f}%)\"\n",
        "\n",
        "        img_btn.click(fn=wrap_face, inputs=img_input, outputs=img_out)\n",
        "\n",
        "    with gr.Tab(\"Speech Emotion\"):\n",
        "        audio_input = gr.Audio(type=\"filepath\", label=\"Upload .wav file\")\n",
        "        audio_btn = gr.Button(\"Analyze Speech Emotion\")\n",
        "        audio_out = gr.Textbox(label=\"Detected Emotion\")\n",
        "\n",
        "        def wrap_audio(path):\n",
        "            emo, conf = classify_speech(path)\n",
        "            return f\"{emo} ({conf*100:.2f}%)\"\n",
        "\n",
        "        audio_btn.click(fn=wrap_audio, inputs=audio_input, outputs=audio_out)\n",
        "\n",
        "    with gr.Tab(\"Fusion (Text + Face + Speech)\"):\n",
        "        fusion_text = gr.Textbox(lines=2, label=\"Enter text\")\n",
        "        fusion_img = gr.Image(type=\"pil\", label=\"Upload Face Image\")\n",
        "        fusion_audio = gr.Audio(type=\"filepath\", label=\"Upload .wav file\")\n",
        "        fusion_btn = gr.Button(\"Run Fusion Model\")\n",
        "        fusion_out = gr.Textbox(label=\"Fusion Result\")\n",
        "        fusion_btn.click(fn=fusion_model, inputs=[fusion_text, fusion_img, fusion_audio], outputs=fusion_out)\n",
        "\n",
        "app.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "NdcmE4thSt43",
        "outputId": "298c36a3-e782-4c64-e887-6b778a7fcf81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://735b62df41e050288c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://735b62df41e050288c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2X2ZvB5NWy1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import torch\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# Load text model components\n",
        "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
        "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
        "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
        "\n",
        "# Load facial model\n",
        "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
        "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "# Load speech emotion model\n",
        "model_name = \"prithivMLmods/Speech-Emotion-Classification\"\n",
        "speech_model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)\n",
        "speech_processor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
        "id2label = {\n",
        "    0: \"Anger\", 1: \"Calm\", 2: \"Disgust\", 3: \"Fear\",\n",
        "    4: \"Happy\", 5: \"Neutral\", 6: \"Sad\", 7: \"Surprised\"\n",
        "}\n",
        "\n",
        "# ---------- TEXT EMOTION FUNCTIONS ----------\n",
        "def clean_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "    text = text.lower().split()\n",
        "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
        "    return \" \".join(text)\n",
        "\n",
        "def predict_text_emotion(input_text):\n",
        "    cleaned_text = clean_text(input_text)\n",
        "    input_vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
        "    predicted_label = lg.predict(input_vectorized)[0]\n",
        "    predicted_emotion = lb.inverse_transform([predicted_label])[0]\n",
        "    probability = np.max(lg.predict_proba(input_vectorized))\n",
        "    return predicted_emotion, probability\n",
        "\n",
        "# ---------- FACIAL EMOTION FUNCTIONS ----------\n",
        "def prepare_image(img_pil):\n",
        "    img = img_pil.convert(\"L\")\n",
        "    img = img.resize((48, 48))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return img_array\n",
        "\n",
        "def detect_facial_emotion(image):\n",
        "    try:\n",
        "        processed_image = prepare_image(image)\n",
        "        prediction = facial_model.predict(processed_image)\n",
        "        predicted_index = np.argmax(prediction)\n",
        "        predicted_class = class_names[predicted_index]\n",
        "        confidence = round(float(prediction[0][predicted_index]), 2)\n",
        "        return predicted_class, confidence\n",
        "    except Exception as e:\n",
        "        return \"Prediction Error\", 0.0\n",
        "\n",
        "# ---------- SPEECH EMOTION FUNCTIONS ----------\n",
        "def detect_speech_emotion(audio_path):\n",
        "    try:\n",
        "        speech, sr = librosa.load(audio_path, sr=16000)\n",
        "        inputs = speech_processor(speech, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
        "        with torch.no_grad():\n",
        "            logits = speech_model(**inputs).logits\n",
        "            probs = torch.nn.functional.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
        "        top_idx = int(np.argmax(probs))\n",
        "        return id2label[top_idx], float(probs[top_idx])\n",
        "    except:\n",
        "        return \"Error\", 0.0\n",
        "\n",
        "# ---------- FUSION LOGIC ----------\n",
        "def fusion_emotion(text, image, audio):\n",
        "    results = {}\n",
        "\n",
        "    # Text prediction\n",
        "    if text:\n",
        "        txt_emotion, txt_conf = predict_text_emotion(text)\n",
        "        results['Text'] = (txt_emotion, txt_conf)\n",
        "\n",
        "    # Facial prediction\n",
        "    if image:\n",
        "        img_emotion, img_conf = detect_facial_emotion(image)\n",
        "        results['Facial'] = (img_emotion, img_conf)\n",
        "\n",
        "    # Speech prediction\n",
        "    if audio:\n",
        "        speech_emotion, speech_conf = detect_speech_emotion(audio)\n",
        "        results['Speech'] = (speech_emotion, speech_conf)\n",
        "\n",
        "    # Fusion logic (weighted or majority based)\n",
        "    fusion_result = \"\"\n",
        "    if results:\n",
        "        # Get dominant based on highest confidence\n",
        "        dominant = max(results.items(), key=lambda x: x[1][1])\n",
        "        fusion_result = f\"Final Emotion: {dominant[1][0]} (from {dominant[0]} modality)\\n\"\n",
        "\n",
        "        # Append all individual results\n",
        "        for k, v in results.items():\n",
        "            fusion_result += f\"{k} Emotion: {v[0]} ({v[1]*100:.2f}%)\\n\"\n",
        "    else:\n",
        "        fusion_result = \"Insufficient data for prediction.\"\n",
        "\n",
        "    return fusion_result.strip()\n",
        "\n",
        "# ---------- GRADIO UI ----------\n",
        "with gr.Blocks(title=\"Multimodal Emotion Detection\") as app:\n",
        "    gr.Markdown(\"\"\"\n",
        "    <h1 style='text-align:center; color:#3B82F6;'>Multimodal Emotion Detection App</h1>\n",
        "    <p style='text-align:center;'>Detect emotions using Text, Facial Image, and Speech Audio</p>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        text_input = gr.Textbox(label=\"Enter Text\", lines=2, placeholder=\"How are you feeling today?\")\n",
        "        image_input = gr.Image(type=\"pil\", label=\"Upload Face Image\")\n",
        "        audio_input = gr.Audio(type=\"filepath\", label=\"Upload Speech Audio\")\n",
        "\n",
        "    submit_button = gr.Button(\"Analyze Emotion\")\n",
        "    output_box = gr.Textbox(label=\"Results\")\n",
        "\n",
        "    submit_button.click(\n",
        "        fn=fusion_emotion,\n",
        "        inputs=[text_input, image_input, audio_input],\n",
        "        outputs=output_box\n",
        "    )\n",
        "\n",
        "app.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "GfaaZaHLWygp",
        "outputId": "f887cadc-3089-4b18-b141-7ed3f546fb96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6a64dc3746cf3f6a1b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6a64dc3746cf3f6a1b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import torch\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from nltk.stem import PorterStemmer\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "\n",
        "# --- Load Models ---\n",
        "# Text models\n",
        "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
        "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
        "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
        "\n",
        "# Facial emotion model\n",
        "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
        "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "# Speech model\n",
        "speech_model_name = \"prithivMLmods/Speech-Emotion-Classification\"\n",
        "speech_model = Wav2Vec2ForSequenceClassification.from_pretrained(speech_model_name)\n",
        "speech_processor = Wav2Vec2FeatureExtractor.from_pretrained(speech_model_name)\n",
        "speech_id2label = {0: \"Anger\", 1: \"Calm\", 2: \"Disgust\", 3: \"Fear\", 4: \"Happy\", 5: \"Neutral\", 6: \"Sad\", 7: \"Surprised\"}\n",
        "\n",
        "# --- Helper Functions ---\n",
        "# Text\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text).lower().split()\n",
        "    return \" \".join([stemmer.stem(word) for word in text if word not in stop_words])\n",
        "\n",
        "def predict_text_emotion(text):\n",
        "    cleaned = clean_text(text)\n",
        "    vect = tfidf_vectorizer.transform([cleaned])\n",
        "    pred = lg.predict(vect)[0]\n",
        "    prob = np.max(lg.predict_proba(vect))\n",
        "    emotion = lb.inverse_transform([pred])[0]\n",
        "    return emotion, float(prob*100)\n",
        "\n",
        "# Facial\n",
        "def prepare_image(img_pil):\n",
        "    img = img_pil.convert(\"L\").resize((48, 48))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    return np.expand_dims(img_array, axis=0)\n",
        "\n",
        "def predict_facial_emotion(image):\n",
        "    img = prepare_image(image)\n",
        "    pred = facial_model.predict(img)\n",
        "    idx = np.argmax(pred)\n",
        "    return class_names[idx], float(pred[0][idx]*100)\n",
        "\n",
        "# Speech\n",
        "def predict_speech_emotion(audio_path):\n",
        "    speech, sr = librosa.load(audio_path, sr=16000)\n",
        "    inputs = speech_processor(speech, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
        "    with torch.no_grad():\n",
        "        logits = speech_model(**inputs).logits\n",
        "        probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "    idx = int(np.argmax(probs))\n",
        "    return speech_id2label[idx], float(probs[idx]*100)\n",
        "\n",
        "# Fusion Logic\n",
        "frustration_labels = {\"Angry\", \"Fear\", \"Disgust\", \"Sad\"}\n",
        "\n",
        "def fusion(text, image, audio):\n",
        "    results = []\n",
        "    if text:\n",
        "        t_label, t_conf = predict_text_emotion(text)\n",
        "        results.append((\"Text\", t_label, t_conf))\n",
        "    if image:\n",
        "        f_label, f_conf = predict_facial_emotion(image)\n",
        "        results.append((\"Facial\", f_label, f_conf))\n",
        "    if audio:\n",
        "        s_label, s_conf = predict_speech_emotion(audio)\n",
        "        results.append((\"Speech\", s_label, s_conf))\n",
        "\n",
        "    # Determine dominant emotion\n",
        "    dominant = max(results, key=lambda x: x[2]) if results else (\"None\", \"None\", 0)\n",
        "\n",
        "    THRESHOLD = 70  # confidence threshold\n",
        "\n",
        "    if dominant[1] in frustration_labels and dominant[2] >= THRESHOLD:\n",
        "      alert = \"üö® Human Agent Required\"\n",
        "    else:\n",
        "      alert = \"‚úÖ Bot Can Handle\"\n",
        "\n",
        "\n",
        "    out = f\"Final Emotion: {dominant[1]} ({dominant[2]:.2f}%) from {dominant[0]} modality\\n\"\n",
        "    out += f\"\\n{'-'*30}\\n\"\n",
        "    for mod, label, conf in results:\n",
        "        out += f\"{mod} Emotion: {label} ({conf:.2f}%)\\n\"\n",
        "    out += f\"\\nStatus: {alert}\"\n",
        "    return out\n",
        "\n",
        "# --- UI ---\n",
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"\"\"\n",
        "    <h2 style='text-align:center;color:#3B82F6;'>Customer Support Emotion Analysis</h2>\n",
        "    <p style='text-align:center;'>Detects user frustration from multimodal inputs and recommends escalation to a human agent if needed.</p>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            text_input = gr.Textbox(label=\"User Message (Text)\")\n",
        "            image_input = gr.Image(type=\"pil\", label=\"Facial Expression (Image)\")\n",
        "            audio_input = gr.Audio(type=\"filepath\", label=\"Voice Recording (Audio)\")\n",
        "            btn = gr.Button(\"Analyze Emotion\")\n",
        "        with gr.Column():\n",
        "            output = gr.Textbox(label=\"Detection Result\", lines=12)\n",
        "\n",
        "    btn.click(fn=fusion, inputs=[text_input, image_input, audio_input], outputs=output)\n",
        "\n",
        "app.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "TVId3j5VXNi5",
        "outputId": "35f4e602-a217-47a8-9d31-9aa35bf2ed30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a87520214184be60e4.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a87520214184be60e4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3kilJJ58a4QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "import torch\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from nltk.stem import PorterStemmer\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# Load models\n",
        "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
        "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
        "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
        "\n",
        "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
        "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "speech_model_name = \"prithivMLmods/Speech-Emotion-Classification\"\n",
        "speech_model = Wav2Vec2ForSequenceClassification.from_pretrained(speech_model_name)\n",
        "speech_processor = Wav2Vec2FeatureExtractor.from_pretrained(speech_model_name)\n",
        "id2label = {\n",
        "    0: \"Anger\", 1: \"Calm\", 2: \"Disgust\", 3: \"Fear\",\n",
        "    4: \"Happy\", 5: \"Neutral\", 6: \"Sad\", 7: \"Surprised\"\n",
        "}\n",
        "\n",
        "# Helper functions\n",
        "def clean_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "    text = text.lower().split()\n",
        "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
        "    return \" \".join(text)\n",
        "\n",
        "def predict_emotion(input_text):\n",
        "    cleaned_text = clean_text(input_text)\n",
        "    input_vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
        "    predicted_proba = lg.predict_proba(input_vectorized)[0]\n",
        "    predicted_label = lg.predict(input_vectorized)[0]\n",
        "    predicted_emotion = lb.inverse_transform([predicted_label])[0]\n",
        "    confidence = np.max(predicted_proba)\n",
        "\n",
        "    # Escalation Logic\n",
        "    if predicted_emotion.lower() in [\"angry\", \"frustration\", \"disgust\"] and confidence > 0.6:\n",
        "        decision = \"üî¥ High frustration detected. Escalating to human agent.\"\n",
        "    else:\n",
        "        decision = \"üü¢ Bot can handle this query.\"\n",
        "\n",
        "    return f\"Emotion: {predicted_emotion}, Probability: {confidence:.2f}\\n{decision}\"\n",
        "\n",
        "\n",
        "def prepare_image(img_pil):\n",
        "    img = img_pil.convert(\"L\")\n",
        "    img = img.resize((48, 48))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return img_array\n",
        "\n",
        "def predict_emotion_face(image):\n",
        "    try:\n",
        "        processed_image = prepare_image(image)\n",
        "        prediction = facial_model.predict(processed_image)\n",
        "        predicted_index = np.argmax(prediction)\n",
        "        predicted_class = class_names[predicted_index]\n",
        "        confidence = round(float(prediction[0][predicted_index]) * 100, 2)\n",
        "\n",
        "        frustration_labels = [\"Angry\", \"Fear\", \"Sad\", \"Disgust\"]\n",
        "        if predicted_class in frustration_labels and confidence >= 70:\n",
        "            alert = \"üö® Human Agent Required\"\n",
        "        else:\n",
        "            alert = \"‚úÖ Bot Can Handle\"\n",
        "\n",
        "        return f\"Emotion: {predicted_class}, Probability: {confidence:.2f}%\", alert\n",
        "    except Exception as e:\n",
        "        return f\"Prediction Error: {str(e)}\", \"‚ö†Ô∏è Error\"\n",
        "\n",
        "def predict_emotion_speech(audio_path):\n",
        "    try:\n",
        "        speech, sr = librosa.load(audio_path, sr=16000)\n",
        "        inputs = speech_processor(speech, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
        "        with torch.no_grad():\n",
        "            logits = speech_model(**inputs).logits\n",
        "            probs = torch.nn.functional.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
        "        top_idx = int(np.argmax(probs))\n",
        "        top_emotion = id2label[top_idx]\n",
        "        top_prob = probs[top_idx] * 100\n",
        "\n",
        "        frustration_labels = [\"Anger\", \"Fear\", \"Sad\", \"Disgust\"]\n",
        "        if top_emotion in frustration_labels and top_prob >= 70:\n",
        "            alert = \"üö® Human Agent Required\"\n",
        "        else:\n",
        "            alert = \"‚úÖ Bot Can Handle\"\n",
        "\n",
        "        return f\"Emotion: {top_emotion}, Probability: {top_prob:.2f}%\", alert\n",
        "    except Exception as e:\n",
        "        return f\"Speech Error: {str(e)}\", \"‚ö†Ô∏è Error\"\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks(title=\"Multimodal Emotion Detection\") as app:\n",
        "    gr.Markdown(\"\"\"\n",
        "    <h1 style='text-align: center; color: #3B82F6;'>Multimodal Emotion Detection App</h1>\n",
        "    <p style='text-align: center;'>Detect emotions from text, face image, or speech to guide bot/human handover.</p>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Tab(\"Text Emotion Detection\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                text_input = gr.Textbox(lines=3, label=\"Enter your text\")\n",
        "                gr.Examples([\n",
        "                    \"I'm really pissed off. Don‚Äôt make me repeat myself again.\",\n",
        "                    \"I‚Äôm tired and frustrated with your service.\",\n",
        "                    \"I‚Äôm very happy with the support!\",\n",
        "                    \"It‚Äôs okay, I guess.\",\n",
        "                    \"I feel neutral.\",\n",
        "                ], inputs=text_input)\n",
        "                text_button = gr.Button(\"Analyze Emotion\")\n",
        "            with gr.Column():\n",
        "                text_output = gr.Textbox(label=\"Predicted Emotion\")\n",
        "                text_alert = gr.Textbox(label=\"Agent Escalation\")\n",
        "        text_button.click(fn=predict_emotion_text, inputs=text_input, outputs=[text_output, text_alert])\n",
        "\n",
        "    with gr.Tab(\"Facial Emotion Detection\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                image_input = gr.Image(type=\"pil\", label=\"Upload a face image\")\n",
        "                image_button = gr.Button(\"Detect Emotion\")\n",
        "            with gr.Column():\n",
        "                image_output = gr.Textbox(label=\"Predicted Emotion\")\n",
        "                image_alert = gr.Textbox(label=\"Agent Escalation\")\n",
        "        image_button.click(fn=predict_emotion_face, inputs=image_input, outputs=[image_output, image_alert])\n",
        "\n",
        "    with gr.Tab(\"Speech Emotion Detection\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                audio_input = gr.Audio(type=\"filepath\", label=\"Upload Speech\")\n",
        "                audio_button = gr.Button(\"Detect Emotion\")\n",
        "            with gr.Column():\n",
        "                audio_output = gr.Textbox(label=\"Predicted Emotion\")\n",
        "                audio_alert = gr.Textbox(label=\"Agent Escalation\")\n",
        "        audio_button.click(fn=predict_emotion_speech, inputs=audio_input, outputs=[audio_output, audio_alert])\n",
        "\n",
        "app.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "gGHQfKPUa4Av",
        "outputId": "6d506938-b3c0-4a4e-858c-038a5c7dc740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6de4d0fbef2329c3d7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6de4d0fbef2329c3d7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "import librosa\n",
        "import torch\n",
        "import soundfile as sf\n",
        "from nltk.stem import PorterStemmer\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# Load text model components\n",
        "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
        "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
        "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
        "\n",
        "# Load facial model\n",
        "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
        "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "# Load speech model\n",
        "model_name = \"prithivMLmods/Speech-Emotion-Classification\"\n",
        "speech_model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)\n",
        "processor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
        "id2label = {\n",
        "    0: \"Anger\", 1: \"Calm\", 2: \"Disgust\", 3: \"Fear\",\n",
        "    4: \"Happy\", 5: \"Neutral\", 6: \"Sad\", 7: \"Surprised\"\n",
        "}\n",
        "\n",
        "# ---------- TEXT EMOTION FUNCTIONS ----------\n",
        "def clean_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "    text = text.lower().split()\n",
        "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
        "    return \" \".join(text)\n",
        "\n",
        "def predict_text_emotion(input_text):\n",
        "    cleaned_text = clean_text(input_text)\n",
        "    input_vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
        "    predicted_label = lg.predict(input_vectorized)[0]\n",
        "    predicted_emotion = lb.inverse_transform([predicted_label])[0]\n",
        "    probability = np.max(lg.predict_proba(input_vectorized)) * 100\n",
        "    return predicted_emotion, round(probability, 2)\n",
        "\n",
        "# ---------- FACIAL EMOTION FUNCTIONS ----------\n",
        "def prepare_image(img_pil):\n",
        "    img = img_pil.convert(\"L\")\n",
        "    img = img.resize((48, 48))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return img_array\n",
        "\n",
        "def detect_facial_emotion(image):\n",
        "    try:\n",
        "        processed_image = prepare_image(image)\n",
        "        prediction = facial_model.predict(processed_image)\n",
        "        predicted_index = np.argmax(prediction)\n",
        "        predicted_class = class_names[predicted_index]\n",
        "        confidence = float(prediction[0][predicted_index]) * 100\n",
        "        return predicted_class, round(confidence, 2)\n",
        "    except Exception as e:\n",
        "        return \"Error\", 0.0\n",
        "\n",
        "# ---------- SPEECH EMOTION FUNCTIONS ----------\n",
        "def detect_speech_emotion(audio_path):\n",
        "    try:\n",
        "        speech, sr = librosa.load(audio_path, sr=16000)\n",
        "        inputs = processor(speech, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
        "        with torch.no_grad():\n",
        "            logits = speech_model(**inputs).logits\n",
        "            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "        top_idx = int(np.argmax(probs))\n",
        "        top_label = id2label[top_idx]\n",
        "        top_confidence = float(probs[top_idx]) * 100\n",
        "        return top_label, round(top_confidence, 2)\n",
        "    except Exception as e:\n",
        "        return \"Error\", 0.0\n",
        "\n",
        "# ---------- ESCALATION LOGIC ----------\n",
        "def should_escalate(emotion, confidence):\n",
        "    escalate_emotions = ['Angry', 'Fear', 'Disgust', 'Frustrated']\n",
        "    return emotion in escalate_emotions and confidence >= 60\n",
        "\n",
        "# ---------- GRADIO UI ----------\n",
        "with gr.Blocks(title=\"Multimodal Customer Support Emotion Analyzer\") as app:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üß† Multimodal Emotion Analyzer for Customer Support\n",
        "    Use **Text**, **Facial Image**, or **Speech** to detect customer emotion.\n",
        "    System decides if escalation to human agent is needed.\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### üìù Text Input\")\n",
        "            text_input = gr.Textbox(lines=3, placeholder=\"Type something...\", label=\"Text\")\n",
        "            text_examples = gr.Examples(\n",
        "                examples=[\n",
        "                    \"I‚Äôm really pissed off. Don‚Äôt make me repeat myself again.\",\n",
        "                    \"Everything is broken again, seriously?\",\n",
        "                    \"Hi, I need help with my order.\",\n",
        "                    \"This is the best support ever!\"\n",
        "                ],\n",
        "                inputs=text_input\n",
        "            )\n",
        "            text_btn = gr.Button(\"Analyze Text\")\n",
        "            text_output = gr.Textbox(label=\"Text Emotion (Confidence%)\")\n",
        "\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### üì∑ Facial Image Input\")\n",
        "            image_input = gr.Image(type=\"pil\", label=\"Upload Face Image\")\n",
        "            image_btn = gr.Button(\"Analyze Image\")\n",
        "            image_output = gr.Textbox(label=\"Facial Emotion (Confidence%)\")\n",
        "\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### üéôÔ∏è Speech Input\")\n",
        "            audio_input = gr.Audio(type=\"filepath\", label=\"Upload Audio\")\n",
        "            audio_btn = gr.Button(\"Analyze Audio\")\n",
        "            audio_output = gr.Textbox(label=\"Speech Emotion (Confidence%)\")\n",
        "\n",
        "    final_decision = gr.Textbox(label=\"üîî Escalation Decision\", interactive=False)\n",
        "\n",
        "    def fusion(text, image, audio):\n",
        "        results = []\n",
        "        if text:\n",
        "            emo, conf = predict_text_emotion(text)\n",
        "            results.append((emo, conf))\n",
        "            text_result = f\"{emo} ({conf}%)\"\n",
        "        else:\n",
        "            text_result = \"No text\"\n",
        "\n",
        "        if image:\n",
        "            emo, conf = detect_facial_emotion(image)\n",
        "            results.append((emo, conf))\n",
        "            image_result = f\"{emo} ({conf}%)\"\n",
        "        else:\n",
        "            image_result = \"No image\"\n",
        "\n",
        "        if audio:\n",
        "            emo, conf = detect_speech_emotion(audio)\n",
        "            results.append((emo, conf))\n",
        "            audio_result = f\"{emo} ({conf}%)\"\n",
        "        else:\n",
        "            audio_result = \"No audio\"\n",
        "\n",
        "        # Escalation logic based on any high-intensity negative emotion\n",
        "        needs_escalation = any(should_escalate(e, c) for e, c in results)\n",
        "        decision = \"üö® Escalate to Human Agent\" if needs_escalation else \"‚úÖ Bot can handle\"\n",
        "\n",
        "        return text_result, image_result, audio_result, decision\n",
        "\n",
        "    gr.Button(\"üîç Final Multimodal Analysis\").click(\n",
        "        fn=fusion,\n",
        "        inputs=[text_input, image_input, audio_input],\n",
        "        outputs=[text_output, image_output, audio_output, final_decision]\n",
        "    )\n",
        "\n",
        "app.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "CMk1troAcM5y",
        "outputId": "6add6e11-5de0-47b9-d0d4-a2c61823df28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5e8e6390c5285a0d6b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5e8e6390c5285a0d6b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R7RdUWj1dKC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "import torch\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from nltk.stem import PorterStemmer\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "\n",
        "# ------------------- SETUP -------------------\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# Text model components\n",
        "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
        "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
        "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
        "\n",
        "# Facial model\n",
        "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
        "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "# Speech model\n",
        "speech_model_name = \"prithivMLmods/Speech-Emotion-Classification\"\n",
        "speech_model = Wav2Vec2ForSequenceClassification.from_pretrained(speech_model_name)\n",
        "speech_processor = Wav2Vec2FeatureExtractor.from_pretrained(speech_model_name)\n",
        "speech_id2label = {\n",
        "    0: \"Anger\", 1: \"Calm\", 2: \"Disgust\", 3: \"Fear\",\n",
        "    4: \"Happy\", 5: \"Neutral\", 6: \"Sad\", 7: \"Surprised\"\n",
        "}\n",
        "\n",
        "# ------------------- TEXT -------------------\n",
        "def clean_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "    text = text.lower().split()\n",
        "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
        "    return \" \".join(text)\n",
        "\n",
        "def predict_text_emotion(text):\n",
        "    if not text.strip():\n",
        "        return None, None\n",
        "    cleaned_text = clean_text(text)\n",
        "    vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
        "    label = lg.predict(vectorized)[0]\n",
        "    prob = np.max(lg.predict_proba(vectorized))\n",
        "    emotion = lb.inverse_transform([label])[0]\n",
        "    return emotion, round(prob * 100, 2)\n",
        "\n",
        "# ------------------- FACE -------------------\n",
        "def prepare_image(img_pil):\n",
        "    img = img_pil.convert(\"L\").resize((48, 48))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    return np.expand_dims(img_array, axis=0)\n",
        "\n",
        "def predict_face_emotion(image):\n",
        "    if image is None:\n",
        "        return None, None\n",
        "    processed = prepare_image(image)\n",
        "    pred = facial_model.predict(processed)\n",
        "    idx = np.argmax(pred)\n",
        "    return class_names[idx], round(float(pred[0][idx]) * 100, 2)\n",
        "\n",
        "# ------------------- SPEECH -------------------\n",
        "def predict_speech_emotion(audio_path):\n",
        "    if audio_path is None:\n",
        "        return None, None\n",
        "    speech, sr = librosa.load(audio_path, sr=16000)\n",
        "    inputs = speech_processor(speech, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
        "    with torch.no_grad():\n",
        "        logits = speech_model(**inputs).logits\n",
        "        probs = torch.nn.functional.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
        "    top_idx = int(np.argmax(probs))\n",
        "    return speech_id2label[top_idx], round(probs[top_idx] * 100, 2)\n",
        "\n",
        "# ------------------- FUSION -------------------\n",
        "def escalation_needed(emotion_conf_list):\n",
        "    \"\"\"Return True if any emotion is Anger, Fear, Disgust with ‚â• 40% confidence\"\"\"\n",
        "    for emotion, confidence in emotion_conf_list:\n",
        "        if emotion in ['Angry', 'Fear', 'Disgust', 'Anger'] and confidence is not None and confidence >= 40:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# ------------------- UI -------------------\n",
        "with gr.Blocks(title=\"Customer Support Escalation System\") as app:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # ü§ñ Customer Emotion Monitoring System\n",
        "    Upload or provide any modality (Text / Facial Image / Audio) and the system will detect emotion and decide whether a bot can handle it or escalation to a human is needed.\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        text_input = gr.Textbox(label=\"Text Input\", lines=3, placeholder=\"e.g., I'm really frustrated with this service!\")\n",
        "        image_input = gr.Image(type=\"pil\", label=\"Upload Face Image\")\n",
        "        audio_input = gr.Audio(type=\"filepath\", label=\"Upload Audio File\")\n",
        "\n",
        "    with gr.Row():\n",
        "        analyze_button = gr.Button(\"Analyze Customer Emotion\")\n",
        "\n",
        "    with gr.Row():\n",
        "        text_out = gr.Textbox(label=\"Text Emotion Result\")\n",
        "        face_out = gr.Textbox(label=\"Facial Emotion Result\")\n",
        "        audio_out = gr.Textbox(label=\"Speech Emotion Result\")\n",
        "\n",
        "    final_decision = gr.Textbox(label=\"System Decision: Escalate to Human or Bot Can Handle\", lines=2)\n",
        "\n",
        "    def analyze_all(text, image, audio):\n",
        "        text_emotion, text_conf = predict_text_emotion(text)\n",
        "        face_emotion, face_conf = predict_face_emotion(image)\n",
        "        speech_emotion, speech_conf = predict_speech_emotion(audio)\n",
        "\n",
        "        text_result = f\"{text_emotion} ({text_conf}%)\" if text_emotion else \"No text input\"\n",
        "        face_result = f\"{face_emotion} ({face_conf}%)\" if face_emotion else \"No image input\"\n",
        "        speech_result = f\"{speech_emotion} ({speech_conf}%)\" if speech_emotion else \"No audio input\"\n",
        "\n",
        "        emotions = [(text_emotion, text_conf), (face_emotion, face_conf), (speech_emotion, speech_conf)]\n",
        "\n",
        "        decision = \"üî¥ Escalate to Human Agent\" if escalation_needed(emotions) else \"üü¢ Bot Can Handle\"\n",
        "\n",
        "        return text_result, face_result, speech_result, decision\n",
        "\n",
        "    analyze_button.click(\n",
        "        fn=analyze_all,\n",
        "        inputs=[text_input, image_input, audio_input],\n",
        "        outputs=[text_out, face_out, audio_out, final_decision]\n",
        "    )\n",
        "\n",
        "app.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "9UVwSmX4dKco",
        "outputId": "2d222280-247c-4a14-f70a-8e94858e2d25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8e0afb39fd8f59eb49.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8e0afb39fd8f59eb49.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "import torch\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from nltk.stem import PorterStemmer\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "\n",
        "# --- Download NLTK stopwords ---\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# --- Load text model components ---\n",
        "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
        "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
        "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
        "\n",
        "# --- Load facial emotion model ---\n",
        "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
        "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "# --- Load speech model ---\n",
        "speech_model_name = \"prithivMLmods/Speech-Emotion-Classification\"\n",
        "speech_model = Wav2Vec2ForSequenceClassification.from_pretrained(speech_model_name)\n",
        "speech_processor = Wav2Vec2FeatureExtractor.from_pretrained(speech_model_name)\n",
        "speech_id2label = {\n",
        "    0: \"Anger\", 1: \"Calm\", 2: \"Disgust\", 3: \"Fear\",\n",
        "    4: \"Happy\", 5: \"Neutral\", 6: \"Sad\", 7: \"Surprised\"\n",
        "}\n",
        "\n",
        "# ---------- TEXT EMOTION FUNCTIONS ----------\n",
        "def clean_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "    text = text.lower().split()\n",
        "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
        "    return \" \".join(text)\n",
        "\n",
        "def predict_text_emotion(input_text):\n",
        "    cleaned_text = clean_text(input_text)\n",
        "    input_vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
        "    predicted_label = lg.predict(input_vectorized)[0]\n",
        "    predicted_emotion = lb.inverse_transform([predicted_label])[0]\n",
        "    probability = np.max(lg.predict_proba(input_vectorized)) * 100\n",
        "    return f\"{predicted_emotion} ({probability:.2f}%)\"\n",
        "\n",
        "# ---------- FACIAL EMOTION FUNCTIONS ----------\n",
        "def prepare_image(img_pil):\n",
        "    img = img_pil.convert(\"L\")\n",
        "    img = img.resize((48, 48))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return img_array\n",
        "\n",
        "def predict_facial_emotion(image):\n",
        "    try:\n",
        "        processed_image = prepare_image(image)\n",
        "        prediction = facial_model.predict(processed_image)\n",
        "        predicted_index = np.argmax(prediction)\n",
        "        predicted_class = class_names[predicted_index]\n",
        "        confidence = float(prediction[0][predicted_index]) * 100\n",
        "        return f\"{predicted_class} ({confidence:.2f}%)\"\n",
        "    except Exception as e:\n",
        "        return f\"Prediction Error: {str(e)}\"\n",
        "\n",
        "# ---------- SPEECH EMOTION FUNCTION ----------\n",
        "def predict_speech_emotion(audio_path):\n",
        "    try:\n",
        "        speech, sr = librosa.load(audio_path, sr=16000)\n",
        "        inputs = speech_processor(speech, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
        "        with torch.no_grad():\n",
        "            logits = speech_model(**inputs).logits\n",
        "            probs = torch.nn.functional.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
        "        top_idx = int(np.argmax(probs))\n",
        "        top_label = speech_id2label[top_idx]\n",
        "        top_prob = float(probs[top_idx]) * 100\n",
        "        return f\"{top_label} ({top_prob:.2f}%)\"\n",
        "    except Exception as e:\n",
        "        return f\"Audio Error: {str(e)}\"\n",
        "\n",
        "# ---------- ESCALATION LOGIC ----------\n",
        "def should_escalate(results):\n",
        "    escalation_emotions = {\"anger\", \"fear\", \"disgust\"}\n",
        "    threshold = 40.0\n",
        "\n",
        "    for result in results:\n",
        "        if result:\n",
        "            try:\n",
        "                emotion, conf = result.split(\"(\")\n",
        "                emotion = emotion.strip().lower()\n",
        "                confidence = float(conf.strip().replace(\"%)\", \"\").replace(\")\", \"\"))\n",
        "                if emotion in escalation_emotions and confidence >= threshold:\n",
        "                    return True\n",
        "            except Exception as e:\n",
        "                print(\"Error parsing result:\", result, \"| Error:\", e)\n",
        "                continue\n",
        "    return False\n",
        "\n",
        "# ---------- FINAL FUSION LOGIC ----------\n",
        "def fused_decision(text_input, image_input, audio_input):\n",
        "    text_result = predict_text_emotion(text_input) if text_input else None\n",
        "    image_result = predict_facial_emotion(image_input) if image_input else None\n",
        "    audio_result = predict_speech_emotion(audio_input) if audio_input else None\n",
        "\n",
        "    should_alert = should_escalate([text_result, image_result, audio_result])\n",
        "    final_decision = \"üî¥ Escalate to human agent\" if should_alert else \"üü¢ Bot can handle\"\n",
        "\n",
        "    return text_result or \"No Text Input\", image_result or \"No Image Input\", audio_result or \"No Audio Input\", final_decision\n",
        "\n",
        "# ---------- UI ----------\n",
        "with gr.Blocks(title=\"Customer Support Emotion Escalation System\") as app:\n",
        "    gr.Markdown(\"\"\"\n",
        "    <h1 style='text-align: center; color: #3B82F6;'>ü§ñ Multimodal Emotion Detection & Escalation</h1>\n",
        "    <p style='text-align: center;'>Analyze customer's emotion via Text, Facial Image, or Voice ‚Äî and decide if human support is needed.</p>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            text_input = gr.Textbox(label=\"Text Input\", placeholder=\"Type something the customer said...\")\n",
        "            gr.Examples(\n",
        "                examples=[\n",
        "                    \"I'm really pissed off right now!\",\n",
        "                    \"Why is your app so buggy and slow?\",\n",
        "                    \"This is absolutely fine, thank you!\"\n",
        "                ],\n",
        "                inputs=text_input\n",
        "            )\n",
        "\n",
        "            image_input = gr.Image(type=\"pil\", label=\"Facial Image Input\")\n",
        "            audio_input = gr.Audio(type=\"filepath\", label=\"Speech Audio Input\")\n",
        "            btn = gr.Button(\"üîç Analyze Emotion\")\n",
        "\n",
        "        with gr.Column():\n",
        "            text_output = gr.Textbox(label=\"Text Emotion\")\n",
        "            image_output = gr.Textbox(label=\"Facial Emotion\")\n",
        "            audio_output = gr.Textbox(label=\"Speech Emotion\")\n",
        "            final_decision = gr.Textbox(label=\"System Decision (Bot or Human)\", lines=1)\n",
        "\n",
        "    btn.click(fn=fused_decision,\n",
        "              inputs=[text_input, image_input, audio_input],\n",
        "              outputs=[text_output, image_output, audio_output, final_decision])\n",
        "\n",
        "app.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7a01d7dcb3df45bb96f0cfb817447856",
            "5e5eb0efff554a4d8688cf75c26ed975",
            "9c9626d63a5746b49edc6f13b8a11b12",
            "3b2f4687744a48f3ab3d6703cf678125",
            "c5fe4ad7622a44d3a10402c0f15033fb",
            "a0c349cf779046289ceb4090a79e303c",
            "287a46b2b3724c44a63b342ac3439de6",
            "f8a0fccd8acc46dab0a2929fb4848591",
            "f5bc68aaef0e4a50802e37a35f7f09ff",
            "40d7340f390f4fd788d6f4cdf22c429e",
            "7d9d6891aa81410faf7474889af514bc",
            "e10804ff5b294e93bc2d1bf89110f70e",
            "877c620806c84430b17b15ae3345dd7c",
            "c59af67155114543850df7388b07e976",
            "6ffe6b6bf72d41408a76460747294e42",
            "ec5976c314e041a391dd771f456ec1da",
            "6d1c9e5261d843ed8d9427c7ace874cd",
            "46c0e61de6ea42e5a7928de588ab5743",
            "7f69a43ff8c94ae290acaa3f4c8a13c4",
            "c83eabfb31064ab28101916426a138d2",
            "a9eb450a4cef48de9d34534e91fb014c",
            "3598e3d8ea3c439ebe556ad11cfc8293",
            "9dc15d1c8bd3428db66b3381dffc0549",
            "8fb34107b1154664bffed38d38a4e252",
            "aa031d43070841c8a3c270efbe4c871e",
            "e77193672cc040e8a762267c68dc036b",
            "ed1bae75803a4f28bc912c5b0d9f9cca",
            "c1bb2149902c44c6be6a3a980e5aaa36",
            "f002913927f84529a06050c635b91867",
            "6c3e1a7607884cecba5c4a48ddc8d3be",
            "03acfabcfb3e418faca6ecdf52cd2405",
            "5c27250555bc4bea9d565edf3331656a",
            "36fe162e32c7464f8e32c58d2af2b452"
          ]
        },
        "id": "Ye_PCLLwgE7r",
        "outputId": "c23aa909-697f-4362-cafc-0fdc22464d97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a01d7dcb3df45bb96f0cfb817447856"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e10804ff5b294e93bc2d1bf89110f70e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9dc15d1c8bd3428db66b3381dffc0549"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://87a96851e44445b381.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://87a96851e44445b381.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU6pA6z7uROV"
      },
      "source": [
        "# Webcam enabled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "zuLrTgVnuVpk",
        "outputId": "cce47474-18fb-44a8-e793-a1933f114a16"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-1-570e23e17851>, line 93)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-570e23e17851>\"\u001b[0;36m, line \u001b[0;32m93\u001b[0m\n\u001b[0;31m    image_button = gr.Button(\"Detect Emotion\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# Load text model components\n",
        "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
        "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
        "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
        "\n",
        "# Load facial model\n",
        "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
        "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "# ---------- TEXT EMOTION FUNCTIONS ----------\n",
        "def clean_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "    text = text.lower().split()\n",
        "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
        "    return \" \".join(text)\n",
        "\n",
        "def predict_emotion(input_text):\n",
        "    cleaned_text = clean_text(input_text)\n",
        "    input_vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
        "    predicted_label = lg.predict(input_vectorized)[0]\n",
        "    predicted_emotion = lb.inverse_transform([predicted_label])[0]\n",
        "    probability = np.max(lg.predict_proba(input_vectorized))\n",
        "    return f\"Emotion: {predicted_emotion}, Probability: {probability:.2f}\"\n",
        "\n",
        "# ---------- FACIAL EMOTION FUNCTIONS ----------\n",
        "def prepare_image(img_pil):\n",
        "    img = img_pil.convert(\"L\")  # Grayscale\n",
        "    img = img.resize((48, 48))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return img_array\n",
        "\n",
        "def detect_emotion(image):\n",
        "    if image is None:\n",
        "        return \"No image captured. Please try again.\"\n",
        "    try:\n",
        "        processed_image = prepare_image(image)\n",
        "        prediction = facial_model.predict(processed_image)\n",
        "        predicted_index = np.argmax(prediction)\n",
        "        predicted_class = class_names[predicted_index]\n",
        "        confidence = round(float(prediction[0][predicted_index]) * 100, 2)\n",
        "        return f\"{predicted_class} ({confidence}%)\"\n",
        "    except Exception as e:\n",
        "        return f\"Prediction Error: {str(e)}\"\n",
        "\n",
        "# ---------- GRADIO UI ----------\n",
        "with gr.Blocks(title=\"Emotion Detection Suite\") as app:\n",
        "    gr.Markdown(\n",
        "        \"<h1 style='text-align: center; color: #3B82F6;'>Multimodal Emotion Detection App</h1>\"\n",
        "        \"<p style='text-align: center;'>Detect emotions from text and facial expressions</p>\"\n",
        "    )\n",
        "\n",
        "    with gr.Tab(\"Text Emotion Detection\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                text_input = gr.Textbox(lines=3, placeholder=\"Type a sentence...\", label=\"Enter your text\")\n",
        "                text_examples = gr.Examples(\n",
        "                    examples=[\n",
        "                        \"I am feeling fantastic today!\",\n",
        "                        \"Why does everything feel so stressful?\",\n",
        "                        \"I'm scared about tomorrow's interview.\",\n",
        "                        \"This makes me so angry!\",\n",
        "                        \"What a pleasant surprise!\",\n",
        "                        \"I‚Äôm just feeling neutral right now.\"\n",
        "                    ],\n",
        "                    inputs=text_input\n",
        "                )\n",
        "                text_button = gr.Button(\"Analyze Emotion\")\n",
        "            with gr.Column():\n",
        "                text_output = gr.Textbox(label=\"Predicted Emotion\")\n",
        "\n",
        "        text_button.click(fn=predict_emotion, inputs=text_input, outputs=text_output)\n",
        "\n",
        "    with gr.Tab(\"Facial Emotion Detection\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "               image_input = gr.Image(source=\"webcam\", tool=\"editor\", type=\"pil\", label=\"Capture your face using webcam\")\n",
        "                image_button = gr.Button(\"Detect Emotion\")\n",
        "            with gr.Column():\n",
        "                image_output = gr.Textbox(label=\"Predicted Emotion\")\n",
        "\n",
        "        image_button.click(fn=detect_emotion, inputs=image_input, outputs=image_output)\n",
        "\n",
        "# Use share=True in Colab or for public access\n",
        "app.launch(share=True, debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "EQ-TPjYpxmQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# Load text model components\n",
        "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
        "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
        "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
        "\n",
        "# Load facial model\n",
        "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
        "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "\n",
        "# ---------- TEXT EMOTION FUNCTIONS ----------\n",
        "def clean_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "    text = text.lower().split()\n",
        "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
        "    return \" \".join(text)\n",
        "\n",
        "def predict_emotion(input_text):\n",
        "    cleaned_text = clean_text(input_text)\n",
        "    input_vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
        "    predicted_label = lg.predict(input_vectorized)[0]\n",
        "    predicted_emotion = lb.inverse_transform([predicted_label])[0]\n",
        "    probability = np.max(lg.predict_proba(input_vectorized))\n",
        "    return f\"Emotion: {predicted_emotion}, Probability: {probability:.2f}\"\n",
        "\n",
        "\n",
        "# ---------- FACIAL EMOTION FUNCTIONS ----------\n",
        "def prepare_image(img_pil):\n",
        "    img = img_pil.convert(\"L\")\n",
        "    img = img.resize((48, 48))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return img_array\n",
        "\n",
        "def detect_emotion(image):\n",
        "    try:\n",
        "        processed_image = prepare_image(image)\n",
        "        prediction = facial_model.predict(processed_image)\n",
        "        predicted_index = np.argmax(prediction)\n",
        "        predicted_class = class_names[predicted_index]\n",
        "        confidence = round(float(prediction[0][predicted_index]) * 100, 2)\n",
        "        return f\"{predicted_class} ({confidence}%)\"\n",
        "    except Exception as e:\n",
        "        return f\"Prediction Error: {str(e)}\"\n",
        "\n",
        "\n",
        "# ---------- GRADIO UI ----------\n",
        "with gr.Blocks(title=\"Emotion Detection Suite\") as app:\n",
        "    gr.Markdown(\n",
        "        \"<h1 style='text-align: center; color: #3B82F6;'>Multimodal Emotion Detection App</h1>\"\n",
        "        \"<p style='text-align: center;'>Detect emotions from text and facial expressions</p>\"\n",
        "    )\n",
        "\n",
        "    with gr.Tab(\"Text Emotion Detection\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                text_input = gr.Textbox(lines=3, placeholder=\"Type a sentence...\", label=\"Enter your text\")\n",
        "                text_examples = gr.Examples(\n",
        "                    examples=[\n",
        "                        \"I am feeling fantastic today!\",\n",
        "                        \"Why does everything feel so stressful?\",\n",
        "                        \"I'm scared about tomorrow's interview.\",\n",
        "                        \"This makes me so angry!\",\n",
        "                        \"What a pleasant surprise!\",\n",
        "                        \"I‚Äôm just feeling neutral right now.\"\n",
        "                    ],\n",
        "                    inputs=text_input\n",
        "                )\n",
        "                text_button = gr.Button(\"Analyze Emotion\")\n",
        "            with gr.Column():\n",
        "                text_output = gr.Textbox(label=\"Predicted Emotion\")\n",
        "\n",
        "        text_button.click(fn=predict_emotion, inputs=text_input, outputs=text_output)\n",
        "\n",
        "with gr.Tab(\"Facial Emotion Detection\"):\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "           image_input = gr.Image(type=\"pil\", tool=\"editor\", label=\"Take a photo (click camera icon)\")\n",
        "\n",
        "           image_button = gr.Button(\"Detect Emotion\")\n",
        "        with gr.Column():\n",
        "            image_output = gr.Textbox(label=\"Predicted Emotion\")\n",
        "\n",
        "    image_button.click(fn=detect_emotion, inputs=image_input, outputs=image_output)\n",
        "\n",
        "app.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "gb_muNdnxsGk",
        "outputId": "d4da6df8-bef9-4129-90ff-3ef707778cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Image.__init__() got an unexpected keyword argument 'tool'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-cb38e21c3d1e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m            \u001b[0mimage_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pil\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"editor\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Take a photo (click camera icon)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m            \u001b[0mimage_button\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mButton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Detect Emotion\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/component_meta.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Image.__init__() got an unexpected keyword argument 'tool'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries if not already installed\n",
        "!pip install --upgrade gradio numpy nltk tensorflow pillow\n",
        "\n",
        "# --- Imports ---\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "# --- Download NLTK Stopwords ---\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# --- Load Text Emotion Model Components ---\n",
        "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
        "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
        "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
        "\n",
        "# --- Load Facial Emotion Detection Model ---\n",
        "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
        "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "\n",
        "# ---------------- TEXT EMOTION FUNCTIONS ----------------\n",
        "def clean_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "    text = text.lower().split()\n",
        "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
        "    return \" \".join(text)\n",
        "\n",
        "def predict_emotion(input_text):\n",
        "    cleaned_text = clean_text(input_text)\n",
        "    input_vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
        "    predicted_label = lg.predict(input_vectorized)[0]\n",
        "    predicted_emotion = lb.inverse_transform([predicted_label])[0]\n",
        "    probability = np.max(lg.predict_proba(input_vectorized))\n",
        "    return f\"Emotion: {predicted_emotion}, Probability: {probability:.2f}\"\n",
        "\n",
        "\n",
        "# ---------------- FACIAL EMOTION FUNCTIONS ----------------\n",
        "def prepare_image(img_pil):\n",
        "    img = img_pil.convert(\"L\")\n",
        "    img = img.resize((48, 48))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return img_array\n",
        "\n",
        "def detect_emotion(image):\n",
        "    try:\n",
        "        processed_image = prepare_image(image)\n",
        "        prediction = facial_model.predict(processed_image)\n",
        "        predicted_index = np.argmax(prediction)\n",
        "        predicted_class = class_names[predicted_index]\n",
        "        confidence = round(float(prediction[0][predicted_index]) * 100, 2)\n",
        "        return f\"{predicted_class} ({confidence}%)\"\n",
        "    except Exception as e:\n",
        "        return f\"Prediction Error: {str(e)}\"\n",
        "\n",
        "\n",
        "# ---------------- GRADIO INTERFACE ----------------\n",
        "with gr.Blocks(title=\"Multimodal Emotion Detection\") as app:\n",
        "    gr.Markdown(\n",
        "        \"<h1 style='text-align: center; color: #4F46E5;'>üé≠ Multimodal Emotion Detection App</h1>\"\n",
        "        \"<p style='text-align: center;'>Detect human emotions using Text or Facial expressions</p>\"\n",
        "    )\n",
        "\n",
        "    with gr.Tab(\"üìù Text Emotion Detection\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                text_input = gr.Textbox(lines=3, placeholder=\"Type a sentence...\", label=\"Enter your text\")\n",
        "                text_examples = gr.Examples(\n",
        "                    examples=[\n",
        "                        \"I am feeling fantastic today!\",\n",
        "                        \"Why does everything feel so stressful?\",\n",
        "                        \"I'm scared about tomorrow's interview.\",\n",
        "                        \"This makes me so angry!\",\n",
        "                        \"What a pleasant surprise!\",\n",
        "                        \"I‚Äôm just feeling neutral right now.\"\n",
        "                    ],\n",
        "                    inputs=text_input\n",
        "                )\n",
        "                text_button = gr.Button(\"Analyze Emotion\")\n",
        "            with gr.Column():\n",
        "                text_output = gr.Textbox(label=\"Predicted Emotion\")\n",
        "        text_button.click(fn=predict_emotion, inputs=text_input, outputs=text_output)\n",
        "\n",
        "    with gr.Tab(\"üì∑ Facial Emotion Detection\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                image_input = gr.Image(source=\"webcam\", type=\"pil\", label=\"Take a photo or upload an image\")\n",
        "                image_button = gr.Button(\"Detect Emotion\")\n",
        "            with gr.Column():\n",
        "                image_output = gr.Textbox(label=\"Predicted Emotion\")\n",
        "        image_button.click(fn=detect_emotion, inputs=image_input, outputs=image_output)\n",
        "\n",
        "app.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QfRwF7N7ySpH",
        "outputId": "0b444085-6e0f-472f-e94a-4d4b7c4b4eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.11)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, tensorboard, ml-dtypes, tensorflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ml-dtypes-0.5.1 numpy-2.1.3 tensorboard-2.19.0 tensorflow-2.19.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ml_dtypes",
                  "tensorflow"
                ]
              },
              "id": "e2902d75ac4744bcaef558c8e371f34d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Image.__init__() got an unexpected keyword argument 'source'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-2b31c12c4a01>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0mimage_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"webcam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pil\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Take a photo or upload an image\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0mimage_button\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mButton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Detect Emotion\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/component_meta.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Image.__init__() got an unexpected keyword argument 'source'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "70255892a7b24d668d58ed0c0c63c2f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2cef7670d08645d4ae5e968f7979bad7",
              "IPY_MODEL_e3fd240723aa4328b8846577891cb69e",
              "IPY_MODEL_ad8a48da34c24fb5abf1ac6e7e76928b"
            ],
            "layout": "IPY_MODEL_987fce1bdad84aa8b441ab51123ca867"
          }
        },
        "2cef7670d08645d4ae5e968f7979bad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c40711c0e1e548389bbca800035f095e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fe682990628048649d28a58eeb05f5ef",
            "value": "preprocessor_config.json:‚Äá100%"
          }
        },
        "e3fd240723aa4328b8846577891cb69e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30325925153c4a5fae6afb53f9c5f4f0",
            "max": 214,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af34c0f5e44f4c339d22d0b7466fe2ba",
            "value": 214
          }
        },
        "ad8a48da34c24fb5abf1ac6e7e76928b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_432e5436df3c4f5d929798ad95ff6262",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ad0cf33e9ab3432184ac2c53bc0d1213",
            "value": "‚Äá214/214‚Äá[00:00&lt;00:00,‚Äá18.6kB/s]"
          }
        },
        "987fce1bdad84aa8b441ab51123ca867": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c40711c0e1e548389bbca800035f095e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe682990628048649d28a58eeb05f5ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30325925153c4a5fae6afb53f9c5f4f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af34c0f5e44f4c339d22d0b7466fe2ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "432e5436df3c4f5d929798ad95ff6262": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad0cf33e9ab3432184ac2c53bc0d1213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70fca7f28301467d860dc6cf4617ad23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_56736bb3ebf44eb38225f2b8e5a068fc",
              "IPY_MODEL_d7cbcffd188844f393ebb7536dd021d2",
              "IPY_MODEL_86b70bbda97940ca86706f38f4499c0b"
            ],
            "layout": "IPY_MODEL_16cf59a900644f15a25ec3f31c793c5a"
          }
        },
        "56736bb3ebf44eb38225f2b8e5a068fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ceb167c4927541b7908b4dd4929ca09f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_abc8b4e8f1eb41ecbf2168fd14e01301",
            "value": "config.json:‚Äá"
          }
        },
        "d7cbcffd188844f393ebb7536dd021d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49689d00a654443ab5d355e757cf76b1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4260b33d7e9843618950a3bace7f1038",
            "value": 1
          }
        },
        "86b70bbda97940ca86706f38f4499c0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ea8f5ba98e34b329895ee401281bb3a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bb16d0d58f8641db8cc0bc99ad3fb858",
            "value": "‚Äá2.28k/?‚Äá[00:00&lt;00:00,‚Äá227kB/s]"
          }
        },
        "16cf59a900644f15a25ec3f31c793c5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ceb167c4927541b7908b4dd4929ca09f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abc8b4e8f1eb41ecbf2168fd14e01301": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49689d00a654443ab5d355e757cf76b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4260b33d7e9843618950a3bace7f1038": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3ea8f5ba98e34b329895ee401281bb3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb16d0d58f8641db8cc0bc99ad3fb858": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "849c506eb9b04dbd8ada43b09f357fae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9dd41c5bb4e3402d8902cafe107c2ce7",
              "IPY_MODEL_89a2a334a2b44abbb10a736d994b4289",
              "IPY_MODEL_509879ea62a84809aa05266c87c71a89"
            ],
            "layout": "IPY_MODEL_aa71764071bf474099b5a749c746b5e7"
          }
        },
        "9dd41c5bb4e3402d8902cafe107c2ce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a02929f19d6545a9a0b44269f7218343",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_42424cb0d84e48b1937a82df3ad8b422",
            "value": "config.json:‚Äá"
          }
        },
        "89a2a334a2b44abbb10a736d994b4289": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1567e38a6b54460b70dd6bd0a214752",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3cd1d603be844446b65c549421357d27",
            "value": 1
          }
        },
        "509879ea62a84809aa05266c87c71a89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3710531e49c46e6b5c5f4e46c6c4889",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_92c52d4a7caa48a2a814dfd0c0a79b27",
            "value": "‚Äá2.62k/?‚Äá[00:00&lt;00:00,‚Äá237kB/s]"
          }
        },
        "aa71764071bf474099b5a749c746b5e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a02929f19d6545a9a0b44269f7218343": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42424cb0d84e48b1937a82df3ad8b422": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1567e38a6b54460b70dd6bd0a214752": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "3cd1d603be844446b65c549421357d27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3710531e49c46e6b5c5f4e46c6c4889": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92c52d4a7caa48a2a814dfd0c0a79b27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "772765d1a82c49ca91d898cc02c52222": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a56fcb59fc584bd4964d630a0ed0ca1e",
              "IPY_MODEL_d1dcebd86ec34e3caae6ed8eb1b466a3",
              "IPY_MODEL_166502e8de944d21ba4248042762790c"
            ],
            "layout": "IPY_MODEL_779e86ea272c46c9822918b038403861"
          }
        },
        "a56fcb59fc584bd4964d630a0ed0ca1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c08fbefe13254c05929993186cd27135",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_82c2dde1351c429ab86e09fe0de639e9",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "d1dcebd86ec34e3caae6ed8eb1b466a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60f9e63a0d3c43ca806a3cc1eee09d53",
            "max": 378308536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_488ed5e6c0f64180a5855288dce1bd79",
            "value": 378308536
          }
        },
        "166502e8de944d21ba4248042762790c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54395a0210204c8cbfa1e062807b9897",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8f4d4f201fa94d43bbb8099807a10b16",
            "value": "‚Äá378M/378M‚Äá[00:18&lt;00:00,‚Äá17.0MB/s]"
          }
        },
        "779e86ea272c46c9822918b038403861": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c08fbefe13254c05929993186cd27135": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82c2dde1351c429ab86e09fe0de639e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60f9e63a0d3c43ca806a3cc1eee09d53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "488ed5e6c0f64180a5855288dce1bd79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54395a0210204c8cbfa1e062807b9897": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f4d4f201fa94d43bbb8099807a10b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96cfac83b0bc4043829e0762ba863816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa4293c74ab14fdaad192c5eea6e305a",
              "IPY_MODEL_46bd1b82f91b4e4da93817fd1fb9b48e",
              "IPY_MODEL_545a6d83c7484b72a4f7bb5c85a24a64"
            ],
            "layout": "IPY_MODEL_8f1e49e7e0ba4d8f9fe47b735e31432f"
          }
        },
        "fa4293c74ab14fdaad192c5eea6e305a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1100dcfc56e41baa634cd50a69ece62",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_62109e980a3b4f708c710669d897adcb",
            "value": "preprocessor_config.json:‚Äá100%"
          }
        },
        "46bd1b82f91b4e4da93817fd1fb9b48e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9bd330c063143638e3f54690df858b8",
            "max": 215,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccab155a0c3846b18d1a34fdbe0db20a",
            "value": 215
          }
        },
        "545a6d83c7484b72a4f7bb5c85a24a64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55da60e5b3c94fa1905589d070935d56",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b80c6fc6ffff4723a865139e433d9fc3",
            "value": "‚Äá215/215‚Äá[00:00&lt;00:00,‚Äá9.36kB/s]"
          }
        },
        "8f1e49e7e0ba4d8f9fe47b735e31432f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1100dcfc56e41baa634cd50a69ece62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62109e980a3b4f708c710669d897adcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9bd330c063143638e3f54690df858b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccab155a0c3846b18d1a34fdbe0db20a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55da60e5b3c94fa1905589d070935d56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b80c6fc6ffff4723a865139e433d9fc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a01d7dcb3df45bb96f0cfb817447856": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e5eb0efff554a4d8688cf75c26ed975",
              "IPY_MODEL_9c9626d63a5746b49edc6f13b8a11b12",
              "IPY_MODEL_3b2f4687744a48f3ab3d6703cf678125"
            ],
            "layout": "IPY_MODEL_c5fe4ad7622a44d3a10402c0f15033fb"
          }
        },
        "5e5eb0efff554a4d8688cf75c26ed975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0c349cf779046289ceb4090a79e303c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_287a46b2b3724c44a63b342ac3439de6",
            "value": "config.json:‚Äá"
          }
        },
        "9c9626d63a5746b49edc6f13b8a11b12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8a0fccd8acc46dab0a2929fb4848591",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f5bc68aaef0e4a50802e37a35f7f09ff",
            "value": 1
          }
        },
        "3b2f4687744a48f3ab3d6703cf678125": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40d7340f390f4fd788d6f4cdf22c429e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7d9d6891aa81410faf7474889af514bc",
            "value": "‚Äá2.62k/?‚Äá[00:00&lt;00:00,‚Äá158kB/s]"
          }
        },
        "c5fe4ad7622a44d3a10402c0f15033fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0c349cf779046289ceb4090a79e303c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "287a46b2b3724c44a63b342ac3439de6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8a0fccd8acc46dab0a2929fb4848591": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f5bc68aaef0e4a50802e37a35f7f09ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40d7340f390f4fd788d6f4cdf22c429e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d9d6891aa81410faf7474889af514bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e10804ff5b294e93bc2d1bf89110f70e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_877c620806c84430b17b15ae3345dd7c",
              "IPY_MODEL_c59af67155114543850df7388b07e976",
              "IPY_MODEL_6ffe6b6bf72d41408a76460747294e42"
            ],
            "layout": "IPY_MODEL_ec5976c314e041a391dd771f456ec1da"
          }
        },
        "877c620806c84430b17b15ae3345dd7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d1c9e5261d843ed8d9427c7ace874cd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_46c0e61de6ea42e5a7928de588ab5743",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "c59af67155114543850df7388b07e976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f69a43ff8c94ae290acaa3f4c8a13c4",
            "max": 378308536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c83eabfb31064ab28101916426a138d2",
            "value": 378308536
          }
        },
        "6ffe6b6bf72d41408a76460747294e42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9eb450a4cef48de9d34534e91fb014c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3598e3d8ea3c439ebe556ad11cfc8293",
            "value": "‚Äá378M/378M‚Äá[00:05&lt;00:00,‚Äá125MB/s]"
          }
        },
        "ec5976c314e041a391dd771f456ec1da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d1c9e5261d843ed8d9427c7ace874cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46c0e61de6ea42e5a7928de588ab5743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f69a43ff8c94ae290acaa3f4c8a13c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c83eabfb31064ab28101916426a138d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a9eb450a4cef48de9d34534e91fb014c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3598e3d8ea3c439ebe556ad11cfc8293": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9dc15d1c8bd3428db66b3381dffc0549": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8fb34107b1154664bffed38d38a4e252",
              "IPY_MODEL_aa031d43070841c8a3c270efbe4c871e",
              "IPY_MODEL_e77193672cc040e8a762267c68dc036b"
            ],
            "layout": "IPY_MODEL_ed1bae75803a4f28bc912c5b0d9f9cca"
          }
        },
        "8fb34107b1154664bffed38d38a4e252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1bb2149902c44c6be6a3a980e5aaa36",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f002913927f84529a06050c635b91867",
            "value": "preprocessor_config.json:‚Äá100%"
          }
        },
        "aa031d43070841c8a3c270efbe4c871e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c3e1a7607884cecba5c4a48ddc8d3be",
            "max": 215,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03acfabcfb3e418faca6ecdf52cd2405",
            "value": 215
          }
        },
        "e77193672cc040e8a762267c68dc036b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c27250555bc4bea9d565edf3331656a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_36fe162e32c7464f8e32c58d2af2b452",
            "value": "‚Äá215/215‚Äá[00:00&lt;00:00,‚Äá20.8kB/s]"
          }
        },
        "ed1bae75803a4f28bc912c5b0d9f9cca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1bb2149902c44c6be6a3a980e5aaa36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f002913927f84529a06050c635b91867": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c3e1a7607884cecba5c4a48ddc8d3be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03acfabcfb3e418faca6ecdf52cd2405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c27250555bc4bea9d565edf3331656a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36fe162e32c7464f8e32c58d2af2b452": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}