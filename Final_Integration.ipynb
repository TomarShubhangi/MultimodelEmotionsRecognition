{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RgNk4N0k8mns",
    "outputId": "7ff3de45-a884-4c2e-9e09-b2f3959b0ec6"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YxTNr-kc65Tj",
    "outputId": "6729d051-3be3-4e4f-dfbd-250ea5400b1e"
   },
   "outputs": [],
   "source": [
    "!pip3 install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0ZuxZUP7PL1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "import gradio as gr\n",
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2HeagAh7rDu"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import cv2\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GCi1txzw7VER",
    "outputId": "75e1ebb3-b386-4716-dee5-9c37661568ce"
   },
   "outputs": [],
   "source": [
    "model = load_model('/content/drive/MyDrive/facial_emotion_detection_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FJ5v6Zlx9Lt5",
    "outputId": "0fc7239e-99f2-4f74-eaac-9776018ce8c8"
   },
   "outputs": [],
   "source": [
    "# Emotion labels dictionary\n",
    "emotion_labels = {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n",
    "index_to_emotion = {v: k for k, v in emotion_labels.items()}\n",
    "index_to_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "ePCAHlwG9M0u",
    "outputId": "d8accdbc-485f-4f01-df25-d1801ddead25"
   },
   "outputs": [],
   "source": [
    "def prepare_image(img_pil):\n",
    "    \"\"\"Preprocess the PIL image to fit your model's input requirements.\"\"\"\n",
    "    # Convert the PIL image to a numpy array with the target size\n",
    "    img = img_pil.resize((224, 224))\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Convert single image to a batch.\n",
    "    img_array /= 255.0  # Rescale pixel values to [0,1], as done during training\n",
    "    return img_array\n",
    "\n",
    "\n",
    "\n",
    "# Define the Gradio interface\n",
    "def predict_emotion(image):\n",
    "    # Preprocess the image\n",
    "    processed_image = prepare_image(image)\n",
    "    # Make prediction using the model\n",
    "    prediction = model.predict(processed_image)\n",
    "    # Get the emotion label with the highest probability\n",
    "    predicted_class = np.argmax(prediction, axis=1)\n",
    "    predicted_emotion = index_to_emotion.get(predicted_class[0], \"Unknown Emotion\")\n",
    "    return predicted_emotion\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=predict_emotion,  # Your prediction function\n",
    "    inputs=gr.Image(type=\"pil\"),  # Input for uploading an image, directly compatible with PIL images\n",
    "    outputs=\"text\",  # Output as text displaying the predicted emotion\n",
    "    title=\"Emotion Detection\",\n",
    "    description=\"Upload an image and see the predicted emotion.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7j6tE-GE9i3j"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    # Resize and normalize as per training\n",
    "    img = cv2.resize(image, (224, 224))  # assuming input shape was 224x224\n",
    "    img = img / 255.0  # normalize\n",
    "    img = np.expand_dims(img, axis=0)  # add batch dimension\n",
    "    return img\n",
    "\n",
    "def predict_emotion(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    img = preprocess_image(image)\n",
    "    preds = model.predict(img)\n",
    "    emotion = emotion_labels[np.argmax(preds)]\n",
    "    confidence = np.max(preds)\n",
    "    return f\"{emotion} ({confidence*100:.2f}%)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "JZWxaYY79nz_",
    "outputId": "011ecce2-e46e-4806-b618-909b11b47d54"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m iface \u001b[38;5;241m=\u001b[39m \u001b[43mgr\u001b[49m\u001b[38;5;241m.\u001b[39mInterface(\n\u001b[0;32m      2\u001b[0m     fn\u001b[38;5;241m=\u001b[39mpredict_emotion,\n\u001b[0;32m      3\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mgr\u001b[38;5;241m.\u001b[39mImage(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpload Face Image\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      4\u001b[0m     outputs\u001b[38;5;241m=\u001b[39mgr\u001b[38;5;241m.\u001b[39mTextbox(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Emotion\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      5\u001b[0m     title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFacial Emotion Detection (ResNet Model)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m iface\u001b[38;5;241m.\u001b[39mlaunch()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gr' is not defined"
     ]
    }
   ],
   "source": [
    "iface = gr.Interface(\n",
    "    fn=predict_emotion,\n",
    "    inputs=gr.Image(type=\"numpy\", label=\"Upload Face Image\"),\n",
    "    outputs=gr.Textbox(label=\"Predicted Emotion\"),\n",
    "    title=\"Facial Emotion Detection (ResNet Model)\"\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "id": "zuGQAETf-a-5",
    "outputId": "b442d843-75b8-4b81-b518-73ab706538d3"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from PIL import Image\n",
    "\n",
    "# Load your model\n",
    "model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")  # Change to your actual model path\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']  # Update if needed\n",
    "\n",
    "# Ensure correct input shape (likely 4096, check your model summary)\n",
    "def prepare_image(img_pil):\n",
    "    img = img_pil.convert(\"L\")             # Convert to grayscale\n",
    "    img = img.resize((64, 64))             # Try 64x64 if model expects 4096: 64x64 = 4096\n",
    "    img_array = img_to_array(img) / 255.0  # Normalize\n",
    "    img_array = img_array.reshape(1, -1)   # Flatten to (1, 4096)\n",
    "    return img_array\n",
    "\n",
    "# Predict function\n",
    "def detect_emotion(image):\n",
    "    try:\n",
    "        processed_image = prepare_image(image)\n",
    "        prediction = model.predict(processed_image)\n",
    "        predicted_index = np.argmax(prediction)\n",
    "        predicted_class = class_names[predicted_index]\n",
    "        confidence = round(prediction[0][predicted_index] * 100, 2)\n",
    "        return f\"{predicted_class} ({confidence}%)\"\n",
    "    except Exception as e:\n",
    "        return f\"Prediction Error: {str(e)}\"\n",
    "\n",
    "# Gradio UI\n",
    "iface = gr.Interface(\n",
    "    fn=detect_emotion,\n",
    "    inputs=gr.Image(type=\"pil\", label=\"Upload a face image to detect emotion.\"),\n",
    "    outputs=gr.Textbox(label=\"Predicted Emotion\"),\n",
    "    title=\"Facial Emotion Detection\",\n",
    "    description=\"Upload a face image and the model will predict the emotion.\",\n",
    "    theme=\"default\"\n",
    ")\n",
    "\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t6fq-hW6dAMD",
    "outputId": "4f497fa7-0db0-4a6a-da0d-bf415582c9f2"
   },
   "outputs": [],
   "source": [
    "print(model.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "EZhZpbR_b9Tq",
    "outputId": "e0872dbb-1e7e-4f87-a192-682632bf679f"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from PIL import Image\n",
    "\n",
    "# Load the trained model and class names\n",
    "model = load_model(\"your_model_path.h5\")  # Replace with your actual model path\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']  # Update if different\n",
    "\n",
    "# Function to preprocess the image for prediction\n",
    "def prepare_image(img_pil):\n",
    "    img = img_pil.convert(\"L\")  # Convert to grayscale\n",
    "    img = img.resize((48, 48))  # Resize to match training input\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = img_array.reshape(1, -1)  # Flatten to shape (1, 2304)\n",
    "    return img_array\n",
    "\n",
    "# Function to predict emotion\n",
    "def detect_emotion(image):\n",
    "    try:\n",
    "        processed_image = prepare_image(image)\n",
    "        prediction = model.predict(processed_image)\n",
    "        predicted_index = np.argmax(prediction)\n",
    "        predicted_class = class_names[predicted_index]\n",
    "        confidence = round(prediction[0][predicted_index] * 100, 2)\n",
    "        return f\"{predicted_class} ({confidence}%)\"\n",
    "    except Exception as e:\n",
    "        return f\"Prediction Error: {str(e)}\"\n",
    "\n",
    "# Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=detect_emotion,\n",
    "    inputs=gr.Image(type=\"pil\", label=\"Upload a face image to detect emotion.\"),\n",
    "    outputs=gr.Textbox(label=\"Predicted Emotion\"),\n",
    "    title=\"Facial Emotion Detection\",\n",
    "    description=\"This app detects emotion from facial expressions using a pre-trained model.\",\n",
    "    theme=\"default\"\n",
    ")\n",
    "\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "id": "NtXxTeaydSLl",
    "outputId": "0e74aae2-06e9-45cd-b7d3-cb672f0de5b1"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from PIL import Image\n",
    "\n",
    "# Load model and class names\n",
    "model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")  # replace with your actual model path\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']  # use your actual labels\n",
    "\n",
    "# Image preprocessing\n",
    "def prepare_image(img_pil):\n",
    "    img = img_pil.convert(\"L\")                  # convert to grayscale\n",
    "    img = img.resize((48, 48))                  # resize to match model input\n",
    "    img_array = img_to_array(img) / 255.0       # normalize\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # shape becomes (1, 48, 48, 1)\n",
    "    return img_array\n",
    "\n",
    "# Emotion detection\n",
    "def detect_emotion(image):\n",
    "    try:\n",
    "        processed_image = prepare_image(image)\n",
    "        prediction = model.predict(processed_image)\n",
    "        predicted_index = np.argmax(prediction)\n",
    "        predicted_class = class_names[predicted_index]\n",
    "        confidence = round(float(prediction[0][predicted_index]) * 100, 2)\n",
    "        return f\"{predicted_class} ({confidence}%)\"\n",
    "    except Exception as e:\n",
    "        return f\"Prediction Error: {str(e)}\"\n",
    "\n",
    "# Gradio Interface\n",
    "iface = gr.Interface(\n",
    "    fn=detect_emotion,\n",
    "    inputs=gr.Image(type=\"pil\", label=\"Upload a face image to detect emotion.\"),\n",
    "    outputs=gr.Textbox(label=\"Predicted Emotion\"),\n",
    "    title=\"Facial Emotion Detection\",\n",
    "    description=\"Upload a face image and the model will predict the emotion.\",\n",
    "    theme=\"default\"\n",
    ")\n",
    "\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "id": "ieeRVcktfSEt",
    "outputId": "334d3de4-3ce4-4fb7-9ccf-64605e86b402"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Load text model components\n",
    "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
    "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
    "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
    "\n",
    "# Load facial model\n",
    "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "\n",
    "# ---------- TEXT EMOTION FUNCTIONS ----------\n",
    "def clean_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.lower().split()\n",
    "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def predict_emotion(input_text):\n",
    "    cleaned_text = clean_text(input_text)\n",
    "    input_vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
    "    predicted_label = lg.predict(input_vectorized)[0]\n",
    "    predicted_emotion = lb.inverse_transform([predicted_label])[0]\n",
    "    probability = np.max(lg.predict_proba(input_vectorized))\n",
    "    return f\"Emotion: {predicted_emotion}, Probability: {probability:.2f}\"\n",
    "\n",
    "\n",
    "# ---------- FACIAL EMOTION FUNCTIONS ----------\n",
    "def prepare_image(img_pil):\n",
    "    img = img_pil.convert(\"L\")\n",
    "    img = img.resize((48, 48))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "def detect_emotion(image):\n",
    "    try:\n",
    "        processed_image = prepare_image(image)\n",
    "        prediction = facial_model.predict(processed_image)\n",
    "        predicted_index = np.argmax(prediction)\n",
    "        predicted_class = class_names[predicted_index]\n",
    "        confidence = round(float(prediction[0][predicted_index]) * 100, 2)\n",
    "        return f\"{predicted_class} ({confidence}%)\"\n",
    "    except Exception as e:\n",
    "        return f\"Prediction Error: {str(e)}\"\n",
    "\n",
    "\n",
    "# ---------- GRADIO UI ----------\n",
    "with gr.Blocks(title=\"Emotion Detection Suite\") as app:\n",
    "    gr.Markdown(\n",
    "        \"<h1 style='text-align: center; color: #3B82F6;'>Multimodal Emotion Detection App</h1>\"\n",
    "        \"<p style='text-align: center;'>Detect emotions from text and facial expressions</p>\"\n",
    "    )\n",
    "\n",
    "    with gr.Tab(\"Text Emotion Detection\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                text_input = gr.Textbox(lines=3, placeholder=\"Type a sentence...\", label=\"Enter your text\")\n",
    "                text_examples = gr.Examples(\n",
    "                    examples=[\n",
    "                        \"I am feeling fantastic today!\",\n",
    "                        \"Why does everything feel so stressful?\",\n",
    "                        \"I'm scared about tomorrow's interview.\",\n",
    "                        \"This makes me so angry!\",\n",
    "                        \"What a pleasant surprise!\",\n",
    "                        \"I’m just feeling neutral right now.\"\n",
    "                    ],\n",
    "                    inputs=text_input\n",
    "                )\n",
    "                text_button = gr.Button(\"Analyze Emotion\")\n",
    "            with gr.Column():\n",
    "                text_output = gr.Textbox(label=\"Predicted Emotion\")\n",
    "\n",
    "        text_button.click(fn=predict_emotion, inputs=text_input, outputs=text_output)\n",
    "\n",
    "    with gr.Tab(\"Facial Emotion Detection\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                image_input = gr.Image(type=\"pil\", label=\"Upload a face image\")\n",
    "                image_button = gr.Button(\"Detect Emotion\")\n",
    "            with gr.Column():\n",
    "                image_output = gr.Textbox(label=\"Predicted Emotion\")\n",
    "\n",
    "        image_button.click(fn=detect_emotion, inputs=image_input, outputs=image_output)\n",
    "\n",
    "app.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkt4NzI4PY-d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "v2jV3gGFPYbM",
    "outputId": "4a801d10-1183-4107-dc61-48410d41ea98"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Load text model components\n",
    "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
    "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
    "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
    "\n",
    "# Load facial model\n",
    "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# ---------- TEXT EMOTION FUNCTIONS ----------\n",
    "def clean_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.lower().split()\n",
    "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def predict_emotion(input_text):\n",
    "    cleaned_text = clean_text(input_text)\n",
    "    input_vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
    "    predicted_label = lg.predict(input_vectorized)[0]\n",
    "    predicted_emotion = lb.inverse_transform([predicted_label])[0]\n",
    "    probability = np.max(lg.predict_proba(input_vectorized))\n",
    "    return f\"Emotion: {predicted_emotion}, Probability: {probability:.2f}\"\n",
    "\n",
    "# ---------- FACIAL EMOTION FUNCTIONS ----------\n",
    "def prepare_image(img_pil):\n",
    "    img = img_pil.convert(\"L\")\n",
    "    img = img.resize((48, 48))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "def detect_emotion(image):\n",
    "    try:\n",
    "        processed_image = prepare_image(image)\n",
    "        prediction = facial_model.predict(processed_image)\n",
    "        predicted_index = np.argmax(prediction)\n",
    "        predicted_class = class_names[predicted_index]\n",
    "        confidence = round(float(prediction[0][predicted_index]) * 100, 2)\n",
    "        return f\"{predicted_class} ({confidence}%)\"\n",
    "    except Exception as e:\n",
    "        return f\"Prediction Error: {str(e)}\"\n",
    "\n",
    "# ---------- FUSION FUNCTION ----------\n",
    "def fusion_emotion_analysis(text, image):\n",
    "    result = \"\"\n",
    "    text_emotion = \"N/A\"\n",
    "    text_conf = 0.0\n",
    "    face_emotion = \"N/A\"\n",
    "    face_conf = 0.0\n",
    "\n",
    "    # Text Prediction\n",
    "    if text:\n",
    "        cleaned_text = clean_text(text)\n",
    "        input_vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
    "        predicted_text_label = lg.predict(input_vectorized)[0]\n",
    "        text_emotion = lb.inverse_transform([predicted_text_label])[0]\n",
    "        text_conf = np.max(lg.predict_proba(input_vectorized))\n",
    "\n",
    "    # Face Prediction\n",
    "    if image:\n",
    "        try:\n",
    "            processed_image = prepare_image(image)\n",
    "            prediction = facial_model.predict(processed_image)\n",
    "            face_index = np.argmax(prediction)\n",
    "            face_emotion = class_names[face_index]\n",
    "            face_conf = float(prediction[0][face_index])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Fusion Logic\n",
    "    if face_conf > text_conf:\n",
    "        dominant = face_emotion\n",
    "    else:\n",
    "        dominant = text_emotion\n",
    "\n",
    "    result += f\"📝 Text Emotion: {text_emotion} ({text_conf:.2f})\\n\"\n",
    "    result += f\"📷 Face Emotion: {face_emotion} ({face_conf:.2f})\\n\"\n",
    "    result += f\"\\n💡 Final Dominant Emotion: **{dominant}**\"\n",
    "\n",
    "    return result\n",
    "\n",
    "# ---------- GRADIO UI ----------\n",
    "with gr.Blocks(title=\"Emotion Detection Suite\") as app:\n",
    "    gr.Markdown(\n",
    "        \"<h1 style='text-align: center; color: #3B82F6;'>Multimodal Emotion Detection App</h1>\"\n",
    "        \"<p style='text-align: center;'>Detect emotions from text and facial expressions</p>\"\n",
    "    )\n",
    "\n",
    "    with gr.Tab(\"Text Emotion Detection\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                text_input = gr.Textbox(lines=3, placeholder=\"Type a sentence...\", label=\"Enter your text\")\n",
    "                text_examples = gr.Examples(\n",
    "                    examples=[\n",
    "                        \"I am feeling fantastic today!\",\n",
    "                        \"Why does everything feel so stressful?\",\n",
    "                        \"I'm scared about tomorrow's interview.\",\n",
    "                        \"This makes me so angry!\",\n",
    "                        \"What a pleasant surprise!\",\n",
    "                        \"I’m just feeling neutral right now.\"\n",
    "                    ],\n",
    "                    inputs=text_input\n",
    "                )\n",
    "                text_button = gr.Button(\"Analyze Emotion\")\n",
    "            with gr.Column():\n",
    "                text_output = gr.Textbox(label=\"Predicted Emotion\")\n",
    "\n",
    "        text_button.click(fn=predict_emotion, inputs=text_input, outputs=text_output)\n",
    "\n",
    "    with gr.Tab(\"Facial Emotion Detection\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                image_input = gr.Image(type=\"pil\", label=\"Upload a face image\")\n",
    "                image_button = gr.Button(\"Detect Emotion\")\n",
    "            with gr.Column():\n",
    "                image_output = gr.Textbox(label=\"Predicted Emotion\")\n",
    "\n",
    "        image_button.click(fn=detect_emotion, inputs=image_input, outputs=image_output)\n",
    "\n",
    "    with gr.Tab(\"🧠 Fusion: Text + Face\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                fusion_text = gr.Textbox(label=\"Enter Text\", lines=2, placeholder=\"Type something...\")\n",
    "                fusion_image = gr.Image(type=\"pil\", label=\"Upload a face image\")\n",
    "                fusion_button = gr.Button(\"Analyze Both\")\n",
    "            with gr.Column():\n",
    "                fusion_output = gr.Textbox(label=\"Fusion Result\", lines=6)\n",
    "\n",
    "        fusion_button.click(fn=fusion_emotion_analysis, inputs=[fusion_text, fusion_image], outputs=fusion_output)\n",
    "\n",
    "app.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSOcrEq1PHOH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 856,
     "referenced_widgets": [
      "70255892a7b24d668d58ed0c0c63c2f0",
      "2cef7670d08645d4ae5e968f7979bad7",
      "e3fd240723aa4328b8846577891cb69e",
      "ad8a48da34c24fb5abf1ac6e7e76928b",
      "987fce1bdad84aa8b441ab51123ca867",
      "c40711c0e1e548389bbca800035f095e",
      "fe682990628048649d28a58eeb05f5ef",
      "30325925153c4a5fae6afb53f9c5f4f0",
      "af34c0f5e44f4c339d22d0b7466fe2ba",
      "432e5436df3c4f5d929798ad95ff6262",
      "ad0cf33e9ab3432184ac2c53bc0d1213",
      "70fca7f28301467d860dc6cf4617ad23",
      "56736bb3ebf44eb38225f2b8e5a068fc",
      "d7cbcffd188844f393ebb7536dd021d2",
      "86b70bbda97940ca86706f38f4499c0b",
      "16cf59a900644f15a25ec3f31c793c5a",
      "ceb167c4927541b7908b4dd4929ca09f",
      "abc8b4e8f1eb41ecbf2168fd14e01301",
      "49689d00a654443ab5d355e757cf76b1",
      "4260b33d7e9843618950a3bace7f1038",
      "3ea8f5ba98e34b329895ee401281bb3a",
      "bb16d0d58f8641db8cc0bc99ad3fb858"
     ]
    },
    "id": "C0vD86LVRR68",
    "outputId": "96ccaaa3-e146-44e2-c027-2d60ca1ed032"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "import torch\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "# ------------------- TEXT EMOTION -------------------\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
    "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
    "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
    "\n",
    "def clean_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.lower().split()\n",
    "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def predict_text_emotion(input_text):\n",
    "    cleaned = clean_text(input_text)\n",
    "    vec = tfidf_vectorizer.transform([cleaned])\n",
    "    label = lg.predict(vec)[0]\n",
    "    emotion = lb.inverse_transform([label])[0]\n",
    "    prob = np.max(lg.predict_proba(vec))\n",
    "    return f\"Emotion: {emotion}, Probability: {prob:.2f}\"\n",
    "\n",
    "# ------------------- FACIAL EMOTION -------------------\n",
    "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
    "face_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "def prepare_image(img):\n",
    "    img = img.convert(\"L\").resize((48, 48))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    return np.expand_dims(img_array, axis=0)\n",
    "\n",
    "def predict_face_emotion(image):\n",
    "    try:\n",
    "        processed = prepare_image(image)\n",
    "        preds = facial_model.predict(processed)\n",
    "        idx = np.argmax(preds)\n",
    "        conf = round(float(preds[0][idx]) * 100, 2)\n",
    "        return f\"{face_labels[idx]} ({conf}%)\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# ------------------- SPEECH EMOTION -------------------\n",
    "hf_model_name = \"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(hf_model_name)\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(hf_model_name)\n",
    "label_mapping = model.config.id2label\n",
    "\n",
    "def classify_speech(audio_path):\n",
    "    try:\n",
    "        speech, sr = librosa.load(audio_path, sr=16000)\n",
    "        inputs = processor(speech, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        predicted_id = torch.argmax(logits, dim=-1).item()\n",
    "        predicted_label = label_mapping[predicted_id]\n",
    "        confidence = torch.softmax(logits, dim=-1)[0][predicted_id].item()\n",
    "        return f\"Emotion: {predicted_label}, Confidence: {confidence:.2f}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# ------------------- GRADIO UI -------------------\n",
    "with gr.Blocks(title=\"Multimodal Emotion Detection\") as app:\n",
    "    gr.Markdown(\"\"\"\n",
    "    <h1 style='text-align: center; color: #3B82F6;'>Multimodal Emotion Detection</h1>\n",
    "    <p style='text-align: center;'>Detect emotions from text, facial images, and speech</p>\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Tab(\"Text Emotion\"):\n",
    "        txt_input = gr.Textbox(lines=2, label=\"Enter text\")\n",
    "        txt_btn = gr.Button(\"Analyze Text Emotion\")\n",
    "        txt_out = gr.Textbox(label=\"Detected Emotion\")\n",
    "        txt_btn.click(fn=predict_text_emotion, inputs=txt_input, outputs=txt_out)\n",
    "\n",
    "    with gr.Tab(\"Facial Emotion\"):\n",
    "        img_input = gr.Image(type=\"pil\", label=\"Upload Face Image\")\n",
    "        img_btn = gr.Button(\"Detect Facial Emotion\")\n",
    "        img_out = gr.Textbox(label=\"Detected Emotion\")\n",
    "        img_btn.click(fn=predict_face_emotion, inputs=img_input, outputs=img_out)\n",
    "\n",
    "    with gr.Tab(\"Speech Emotion\"):\n",
    "        audio_input = gr.Audio(type=\"filepath\", label=\"Upload .wav file\")\n",
    "        audio_btn = gr.Button(\"Analyze Speech Emotion\")\n",
    "        audio_out = gr.Textbox(label=\"Detected Emotion\")\n",
    "        audio_btn.click(fn=classify_speech, inputs=audio_input, outputs=audio_out)\n",
    "\n",
    "app.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 794,
     "referenced_widgets": [
      "849c506eb9b04dbd8ada43b09f357fae",
      "9dd41c5bb4e3402d8902cafe107c2ce7",
      "89a2a334a2b44abbb10a736d994b4289",
      "509879ea62a84809aa05266c87c71a89",
      "aa71764071bf474099b5a749c746b5e7",
      "a02929f19d6545a9a0b44269f7218343",
      "42424cb0d84e48b1937a82df3ad8b422",
      "e1567e38a6b54460b70dd6bd0a214752",
      "3cd1d603be844446b65c549421357d27",
      "c3710531e49c46e6b5c5f4e46c6c4889",
      "92c52d4a7caa48a2a814dfd0c0a79b27",
      "772765d1a82c49ca91d898cc02c52222",
      "a56fcb59fc584bd4964d630a0ed0ca1e",
      "d1dcebd86ec34e3caae6ed8eb1b466a3",
      "166502e8de944d21ba4248042762790c",
      "779e86ea272c46c9822918b038403861",
      "c08fbefe13254c05929993186cd27135",
      "82c2dde1351c429ab86e09fe0de639e9",
      "60f9e63a0d3c43ca806a3cc1eee09d53",
      "488ed5e6c0f64180a5855288dce1bd79",
      "54395a0210204c8cbfa1e062807b9897",
      "8f4d4f201fa94d43bbb8099807a10b16",
      "96cfac83b0bc4043829e0762ba863816",
      "fa4293c74ab14fdaad192c5eea6e305a",
      "46bd1b82f91b4e4da93817fd1fb9b48e",
      "545a6d83c7484b72a4f7bb5c85a24a64",
      "8f1e49e7e0ba4d8f9fe47b735e31432f",
      "e1100dcfc56e41baa634cd50a69ece62",
      "62109e980a3b4f708c710669d897adcb",
      "d9bd330c063143638e3f54690df858b8",
      "ccab155a0c3846b18d1a34fdbe0db20a",
      "55da60e5b3c94fa1905589d070935d56",
      "b80c6fc6ffff4723a865139e433d9fc3"
     ]
    },
    "id": "K8g5lUpBR2Sq",
    "outputId": "49649aad-c2b8-44c8-e8ca-b437c6981b64"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import torch\n",
    "import librosa\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "\n",
    "# ------------------- TEXT EMOTION -------------------\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
    "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
    "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
    "\n",
    "def clean_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.lower().split()\n",
    "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def predict_text_emotion(input_text):\n",
    "    cleaned = clean_text(input_text)\n",
    "    vec = tfidf_vectorizer.transform([cleaned])\n",
    "    label = lg.predict(vec)[0]\n",
    "    emotion = lb.inverse_transform([label])[0]\n",
    "    prob = np.max(lg.predict_proba(vec))\n",
    "    return f\"Emotion: {emotion}, Probability: {prob:.2f}\"\n",
    "\n",
    "# ------------------- FACIAL EMOTION -------------------\n",
    "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
    "face_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "def prepare_image(img):\n",
    "    img = img.convert(\"L\").resize((48, 48))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    return np.expand_dims(img_array, axis=0)\n",
    "\n",
    "def predict_face_emotion(image):\n",
    "    try:\n",
    "        processed = prepare_image(image)\n",
    "        preds = facial_model.predict(processed)\n",
    "        idx = np.argmax(preds)\n",
    "        conf = round(float(preds[0][idx]) * 100, 2)\n",
    "        return f\"{face_labels[idx]} ({conf}%)\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# ------------------- SPEECH EMOTION -------------------\n",
    "model_name = \"prithivMLmods/Speech-Emotion-Classification\"\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "\n",
    "id2label = {\n",
    "    0: \"Anger\", 1: \"Calm\", 2: \"Disgust\", 3: \"Fear\",\n",
    "    4: \"Happy\", 5: \"Neutral\", 6: \"Sad\", 7: \"Surprised\"\n",
    "}\n",
    "\n",
    "def classify_speech(audio_path):\n",
    "    try:\n",
    "        speech, sr = librosa.load(audio_path, sr=16000)\n",
    "        inputs = processor(speech, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "        top_idx = int(np.argmax(probs))\n",
    "        return {id2label[i]: float(probs[i]) for i in range(len(probs))}\n",
    "    except Exception as e:\n",
    "        return {\"Error\": str(e)}\n",
    "\n",
    "# ------------------- GRADIO UI -------------------\n",
    "with gr.Blocks(title=\"Multimodal Emotion Detection\") as app:\n",
    "    gr.Markdown(\"\"\"\n",
    "    <h1 style='text-align: center; color: #3B82F6;'>Multimodal Emotion Detection</h1>\n",
    "    <p style='text-align: center;'>Detect emotions from text, facial expressions, and speech</p>\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Tab(\"Text Emotion\"):\n",
    "        txt_input = gr.Textbox(lines=2, label=\"Enter text\")\n",
    "        txt_btn = gr.Button(\"Analyze Text Emotion\")\n",
    "        txt_out = gr.Textbox(label=\"Detected Emotion\")\n",
    "        txt_btn.click(fn=predict_text_emotion, inputs=txt_input, outputs=txt_out)\n",
    "\n",
    "    with gr.Tab(\"Facial Emotion\"):\n",
    "        img_input = gr.Image(type=\"pil\", label=\"Upload Face Image\")\n",
    "        img_btn = gr.Button(\"Detect Facial Emotion\")\n",
    "        img_out = gr.Textbox(label=\"Detected Emotion\")\n",
    "        img_btn.click(fn=predict_face_emotion, inputs=img_input, outputs=img_out)\n",
    "\n",
    "    with gr.Tab(\"Speech Emotion\"):\n",
    "        audio_input = gr.Audio(type=\"filepath\", label=\"Upload .wav file\")\n",
    "        audio_btn = gr.Button(\"Analyze Speech Emotion\")\n",
    "        audio_out = gr.Label(num_top_classes=3, label=\"Detected Emotion & Confidence\")\n",
    "        audio_btn.click(fn=classify_speech, inputs=audio_input, outputs=audio_out)\n",
    "\n",
    "app.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rE_M67ZWSuJx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "id": "NdcmE4thSt43",
    "outputId": "298c36a3-e782-4c64-e887-6b778a7fcf81"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import torch\n",
    "import librosa\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "\n",
    "# ------------------- TEXT EMOTION -------------------\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
    "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
    "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
    "\n",
    "def clean_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.lower().split()\n",
    "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def predict_text_emotion(input_text):\n",
    "    cleaned = clean_text(input_text)\n",
    "    vec = tfidf_vectorizer.transform([cleaned])\n",
    "    label = lg.predict(vec)[0]\n",
    "    emotion = lb.inverse_transform([label])[0]\n",
    "    prob = np.max(lg.predict_proba(vec))\n",
    "    return emotion, float(prob)\n",
    "\n",
    "# ------------------- FACIAL EMOTION -------------------\n",
    "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
    "face_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "def prepare_image(img):\n",
    "    img = img.convert(\"L\").resize((48, 48))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    return np.expand_dims(img_array, axis=0)\n",
    "\n",
    "def predict_face_emotion(image):\n",
    "    try:\n",
    "        processed = prepare_image(image)\n",
    "        preds = facial_model.predict(processed)\n",
    "        idx = np.argmax(preds)\n",
    "        conf = round(float(preds[0][idx]), 2)\n",
    "        return face_labels[idx], conf\n",
    "    except Exception as e:\n",
    "        return \"Error\", 0.0\n",
    "\n",
    "# ------------------- SPEECH EMOTION -------------------\n",
    "model_name = \"prithivMLmods/Speech-Emotion-Classification\"\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "\n",
    "id2label = {\n",
    "    0: \"Anger\", 1: \"Calm\", 2: \"Disgust\", 3: \"Fear\",\n",
    "    4: \"Happy\", 5: \"Neutral\", 6: \"Sad\", 7: \"Surprised\"\n",
    "}\n",
    "\n",
    "def classify_speech(audio_path):\n",
    "    try:\n",
    "        speech, sr = librosa.load(audio_path, sr=16000)\n",
    "        inputs = processor(speech, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "        top_idx = int(np.argmax(probs))\n",
    "        return id2label[top_idx], float(probs[top_idx])\n",
    "    except Exception as e:\n",
    "        return \"Error\", 0.0\n",
    "\n",
    "# ------------------- FUSION FUNCTION -------------------\n",
    "def fusion_model(text, image, audio):\n",
    "    text_emo, text_prob = predict_text_emotion(text) if text else (\"N/A\", 0)\n",
    "    face_emo, face_prob = predict_face_emotion(image) if image else (\"N/A\", 0)\n",
    "    speech_emo, speech_prob = classify_speech(audio) if audio else (\"N/A\", 0)\n",
    "\n",
    "    results = {\n",
    "        \"Text\": (text_emo, text_prob),\n",
    "        \"Facial\": (face_emo, face_prob),\n",
    "        \"Speech\": (speech_emo, speech_prob)\n",
    "    }\n",
    "    dominant = max(results.items(), key=lambda x: x[1][1])\n",
    "    fusion_summary = f\"\"\"\n",
    "Text Emotion: {text_emo} ({text_prob*100:.2f}%)\\n\n",
    "Facial Emotion: {face_emo} ({face_prob*100:.2f}%)\\n\n",
    "Speech Emotion: {speech_emo} ({speech_prob*100:.2f}%)\\n\n",
    "\n",
    "👉 Dominant Emotion: **{dominant[0]} - {dominant[1][0]}**\n",
    "\"\"\"\n",
    "    return fusion_summary\n",
    "\n",
    "# ------------------- GRADIO UI -------------------\n",
    "with gr.Blocks(title=\"Multimodal Emotion Detection\") as app:\n",
    "    gr.Markdown(\"\"\"\n",
    "    <h1 style='text-align: center; color: #3B82F6;'>Multimodal Emotion Detection</h1>\n",
    "    <p style='text-align: center;'>Detect emotions from text, facial expressions, and speech</p>\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Tab(\"Text Emotion\"):\n",
    "        txt_input = gr.Textbox(lines=2, label=\"Enter text\")\n",
    "        txt_examples = gr.Examples(\n",
    "            examples=[\n",
    "                \"I am feeling fantastic today!\",\n",
    "                \"Why does everything feel so stressful?\",\n",
    "                \"I'm scared about tomorrow's interview.\",\n",
    "                \"This makes me so angry!\",\n",
    "                \"What a pleasant surprise!\",\n",
    "                \"I’m just feeling neutral right now.\"\n",
    "            ],\n",
    "            inputs=txt_input\n",
    "        )\n",
    "        txt_btn = gr.Button(\"Analyze Text Emotion\")\n",
    "        txt_out = gr.Textbox(label=\"Detected Emotion\")\n",
    "\n",
    "        def wrap_text(text):\n",
    "            emotion, prob = predict_text_emotion(text)\n",
    "            return f\"Emotion: {emotion}, Probability: {prob:.2f}\"\n",
    "\n",
    "        txt_btn.click(fn=wrap_text, inputs=txt_input, outputs=txt_out)\n",
    "\n",
    "    with gr.Tab(\"Facial Emotion\"):\n",
    "        img_input = gr.Image(type=\"pil\", label=\"Upload Face Image\")\n",
    "        img_btn = gr.Button(\"Detect Facial Emotion\")\n",
    "        img_out = gr.Textbox(label=\"Detected Emotion\")\n",
    "\n",
    "        def wrap_face(img):\n",
    "            emo, conf = predict_face_emotion(img)\n",
    "            return f\"{emo} ({conf*100:.2f}%)\"\n",
    "\n",
    "        img_btn.click(fn=wrap_face, inputs=img_input, outputs=img_out)\n",
    "\n",
    "    with gr.Tab(\"Speech Emotion\"):\n",
    "        audio_input = gr.Audio(type=\"filepath\", label=\"Upload .wav file\")\n",
    "        audio_btn = gr.Button(\"Analyze Speech Emotion\")\n",
    "        audio_out = gr.Textbox(label=\"Detected Emotion\")\n",
    "\n",
    "        def wrap_audio(path):\n",
    "            emo, conf = classify_speech(path)\n",
    "            return f\"{emo} ({conf*100:.2f}%)\"\n",
    "\n",
    "        audio_btn.click(fn=wrap_audio, inputs=audio_input, outputs=audio_out)\n",
    "\n",
    "    with gr.Tab(\"Fusion (Text + Face + Speech)\"):\n",
    "        fusion_text = gr.Textbox(lines=2, label=\"Enter text\")\n",
    "        fusion_img = gr.Image(type=\"pil\", label=\"Upload Face Image\")\n",
    "        fusion_audio = gr.Audio(type=\"filepath\", label=\"Upload .wav file\")\n",
    "        fusion_btn = gr.Button(\"Run Fusion Model\")\n",
    "        fusion_out = gr.Textbox(label=\"Fusion Result\")\n",
    "        fusion_btn.click(fn=fusion_model, inputs=[fusion_text, fusion_img, fusion_audio], outputs=fusion_out)\n",
    "\n",
    "app.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2X2ZvB5NWy1n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "id": "GfaaZaHLWygp",
    "outputId": "f887cadc-3089-4b18-b141-7ed3f546fb96"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import torch\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Load text model components\n",
    "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
    "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
    "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
    "\n",
    "# Load facial model\n",
    "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Load speech emotion model\n",
    "model_name = \"prithivMLmods/Speech-Emotion-Classification\"\n",
    "speech_model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)\n",
    "speech_processor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "id2label = {\n",
    "    0: \"Anger\", 1: \"Calm\", 2: \"Disgust\", 3: \"Fear\",\n",
    "    4: \"Happy\", 5: \"Neutral\", 6: \"Sad\", 7: \"Surprised\"\n",
    "}\n",
    "\n",
    "# ---------- TEXT EMOTION FUNCTIONS ----------\n",
    "def clean_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.lower().split()\n",
    "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def predict_text_emotion(input_text):\n",
    "    cleaned_text = clean_text(input_text)\n",
    "    input_vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
    "    predicted_label = lg.predict(input_vectorized)[0]\n",
    "    predicted_emotion = lb.inverse_transform([predicted_label])[0]\n",
    "    probability = np.max(lg.predict_proba(input_vectorized))\n",
    "    return predicted_emotion, probability\n",
    "\n",
    "# ---------- FACIAL EMOTION FUNCTIONS ----------\n",
    "def prepare_image(img_pil):\n",
    "    img = img_pil.convert(\"L\")\n",
    "    img = img.resize((48, 48))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "def detect_facial_emotion(image):\n",
    "    try:\n",
    "        processed_image = prepare_image(image)\n",
    "        prediction = facial_model.predict(processed_image)\n",
    "        predicted_index = np.argmax(prediction)\n",
    "        predicted_class = class_names[predicted_index]\n",
    "        confidence = round(float(prediction[0][predicted_index]), 2)\n",
    "        return predicted_class, confidence\n",
    "    except Exception as e:\n",
    "        return \"Prediction Error\", 0.0\n",
    "\n",
    "# ---------- SPEECH EMOTION FUNCTIONS ----------\n",
    "def detect_speech_emotion(audio_path):\n",
    "    try:\n",
    "        speech, sr = librosa.load(audio_path, sr=16000)\n",
    "        inputs = speech_processor(speech, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "        with torch.no_grad():\n",
    "            logits = speech_model(**inputs).logits\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "        top_idx = int(np.argmax(probs))\n",
    "        return id2label[top_idx], float(probs[top_idx])\n",
    "    except:\n",
    "        return \"Error\", 0.0\n",
    "\n",
    "# ---------- FUSION LOGIC ----------\n",
    "def fusion_emotion(text, image, audio):\n",
    "    results = {}\n",
    "\n",
    "    # Text prediction\n",
    "    if text:\n",
    "        txt_emotion, txt_conf = predict_text_emotion(text)\n",
    "        results['Text'] = (txt_emotion, txt_conf)\n",
    "\n",
    "    # Facial prediction\n",
    "    if image:\n",
    "        img_emotion, img_conf = detect_facial_emotion(image)\n",
    "        results['Facial'] = (img_emotion, img_conf)\n",
    "\n",
    "    # Speech prediction\n",
    "    if audio:\n",
    "        speech_emotion, speech_conf = detect_speech_emotion(audio)\n",
    "        results['Speech'] = (speech_emotion, speech_conf)\n",
    "\n",
    "    # Fusion logic (weighted or majority based)\n",
    "    fusion_result = \"\"\n",
    "    if results:\n",
    "        # Get dominant based on highest confidence\n",
    "        dominant = max(results.items(), key=lambda x: x[1][1])\n",
    "        fusion_result = f\"Final Emotion: {dominant[1][0]} (from {dominant[0]} modality)\\n\"\n",
    "\n",
    "        # Append all individual results\n",
    "        for k, v in results.items():\n",
    "            fusion_result += f\"{k} Emotion: {v[0]} ({v[1]*100:.2f}%)\\n\"\n",
    "    else:\n",
    "        fusion_result = \"Insufficient data for prediction.\"\n",
    "\n",
    "    return fusion_result.strip()\n",
    "\n",
    "# ---------- GRADIO UI ----------\n",
    "with gr.Blocks(title=\"Multimodal Emotion Detection\") as app:\n",
    "    gr.Markdown(\"\"\"\n",
    "    <h1 style='text-align:center; color:#3B82F6;'>Multimodal Emotion Detection App</h1>\n",
    "    <p style='text-align:center;'>Detect emotions using Text, Facial Image, and Speech Audio</p>\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        text_input = gr.Textbox(label=\"Enter Text\", lines=2, placeholder=\"How are you feeling today?\")\n",
    "        image_input = gr.Image(type=\"pil\", label=\"Upload Face Image\")\n",
    "        audio_input = gr.Audio(type=\"filepath\", label=\"Upload Speech Audio\")\n",
    "\n",
    "    submit_button = gr.Button(\"Analyze Emotion\")\n",
    "    output_box = gr.Textbox(label=\"Results\")\n",
    "\n",
    "    submit_button.click(\n",
    "        fn=fusion_emotion,\n",
    "        inputs=[text_input, image_input, audio_input],\n",
    "        outputs=output_box\n",
    "    )\n",
    "\n",
    "app.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "id": "TVId3j5VXNi5",
    "outputId": "35f4e602-a217-47a8-9d31-9aa35bf2ed30"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import torch\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from nltk.stem import PorterStemmer\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "\n",
    "# --- Load Models ---\n",
    "# Text models\n",
    "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
    "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
    "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
    "\n",
    "# Facial emotion model\n",
    "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Speech model\n",
    "speech_model_name = \"prithivMLmods/Speech-Emotion-Classification\"\n",
    "speech_model = Wav2Vec2ForSequenceClassification.from_pretrained(speech_model_name)\n",
    "speech_processor = Wav2Vec2FeatureExtractor.from_pretrained(speech_model_name)\n",
    "speech_id2label = {0: \"Anger\", 1: \"Calm\", 2: \"Disgust\", 3: \"Fear\", 4: \"Happy\", 5: \"Neutral\", 6: \"Sad\", 7: \"Surprised\"}\n",
    "\n",
    "# --- Helper Functions ---\n",
    "# Text\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text).lower().split()\n",
    "    return \" \".join([stemmer.stem(word) for word in text if word not in stop_words])\n",
    "\n",
    "def predict_text_emotion(text):\n",
    "    cleaned = clean_text(text)\n",
    "    vect = tfidf_vectorizer.transform([cleaned])\n",
    "    pred = lg.predict(vect)[0]\n",
    "    prob = np.max(lg.predict_proba(vect))\n",
    "    emotion = lb.inverse_transform([pred])[0]\n",
    "    return emotion, float(prob*100)\n",
    "\n",
    "# Facial\n",
    "def prepare_image(img_pil):\n",
    "    img = img_pil.convert(\"L\").resize((48, 48))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    return np.expand_dims(img_array, axis=0)\n",
    "\n",
    "def predict_facial_emotion(image):\n",
    "    img = prepare_image(image)\n",
    "    pred = facial_model.predict(img)\n",
    "    idx = np.argmax(pred)\n",
    "    return class_names[idx], float(pred[0][idx]*100)\n",
    "\n",
    "# Speech\n",
    "def predict_speech_emotion(audio_path):\n",
    "    speech, sr = librosa.load(audio_path, sr=16000)\n",
    "    inputs = speech_processor(speech, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        logits = speech_model(**inputs).logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "    idx = int(np.argmax(probs))\n",
    "    return speech_id2label[idx], float(probs[idx]*100)\n",
    "\n",
    "# Fusion Logic\n",
    "frustration_labels = {\"Angry\", \"Fear\", \"Disgust\", \"Sad\"}\n",
    "\n",
    "def fusion(text, image, audio):\n",
    "    results = []\n",
    "    if text:\n",
    "        t_label, t_conf = predict_text_emotion(text)\n",
    "        results.append((\"Text\", t_label, t_conf))\n",
    "    if image:\n",
    "        f_label, f_conf = predict_facial_emotion(image)\n",
    "        results.append((\"Facial\", f_label, f_conf))\n",
    "    if audio:\n",
    "        s_label, s_conf = predict_speech_emotion(audio)\n",
    "        results.append((\"Speech\", s_label, s_conf))\n",
    "\n",
    "    # Determine dominant emotion\n",
    "    dominant = max(results, key=lambda x: x[2]) if results else (\"None\", \"None\", 0)\n",
    "\n",
    "    THRESHOLD = 70  # confidence threshold\n",
    "\n",
    "    if dominant[1] in frustration_labels and dominant[2] >= THRESHOLD:\n",
    "      alert = \"🚨 Human Agent Required\"\n",
    "    else:\n",
    "      alert = \"✅ Bot Can Handle\"\n",
    "\n",
    "\n",
    "    out = f\"Final Emotion: {dominant[1]} ({dominant[2]:.2f}%) from {dominant[0]} modality\\n\"\n",
    "    out += f\"\\n{'-'*30}\\n\"\n",
    "    for mod, label, conf in results:\n",
    "        out += f\"{mod} Emotion: {label} ({conf:.2f}%)\\n\"\n",
    "    out += f\"\\nStatus: {alert}\"\n",
    "    return out\n",
    "\n",
    "# --- UI ---\n",
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"\"\"\n",
    "    <h2 style='text-align:center;color:#3B82F6;'>Customer Support Emotion Analysis</h2>\n",
    "    <p style='text-align:center;'>Detects user frustration from multimodal inputs and recommends escalation to a human agent if needed.</p>\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            text_input = gr.Textbox(label=\"User Message (Text)\")\n",
    "            image_input = gr.Image(type=\"pil\", label=\"Facial Expression (Image)\")\n",
    "            audio_input = gr.Audio(type=\"filepath\", label=\"Voice Recording (Audio)\")\n",
    "            btn = gr.Button(\"Analyze Emotion\")\n",
    "        with gr.Column():\n",
    "            output = gr.Textbox(label=\"Detection Result\", lines=12)\n",
    "\n",
    "    btn.click(fn=fusion, inputs=[text_input, image_input, audio_input], outputs=output)\n",
    "\n",
    "app.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kilJJ58a4QO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "id": "gGHQfKPUa4Av",
    "outputId": "6d506938-b3c0-4a4e-858c-038a5c7dc740"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "import torch\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from nltk.stem import PorterStemmer\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Load models\n",
    "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
    "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
    "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
    "\n",
    "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "speech_model_name = \"prithivMLmods/Speech-Emotion-Classification\"\n",
    "speech_model = Wav2Vec2ForSequenceClassification.from_pretrained(speech_model_name)\n",
    "speech_processor = Wav2Vec2FeatureExtractor.from_pretrained(speech_model_name)\n",
    "id2label = {\n",
    "    0: \"Anger\", 1: \"Calm\", 2: \"Disgust\", 3: \"Fear\",\n",
    "    4: \"Happy\", 5: \"Neutral\", 6: \"Sad\", 7: \"Surprised\"\n",
    "}\n",
    "\n",
    "# Helper functions\n",
    "def clean_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.lower().split()\n",
    "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def predict_emotion(input_text):\n",
    "    cleaned_text = clean_text(input_text)\n",
    "    input_vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
    "    predicted_proba = lg.predict_proba(input_vectorized)[0]\n",
    "    predicted_label = lg.predict(input_vectorized)[0]\n",
    "    predicted_emotion = lb.inverse_transform([predicted_label])[0]\n",
    "    confidence = np.max(predicted_proba)\n",
    "\n",
    "    # Escalation Logic\n",
    "    if predicted_emotion.lower() in [\"angry\", \"frustration\", \"disgust\"] and confidence > 0.6:\n",
    "        decision = \"🔴 High frustration detected. Escalating to human agent.\"\n",
    "    else:\n",
    "        decision = \"🟢 Bot can handle this query.\"\n",
    "\n",
    "    return f\"Emotion: {predicted_emotion}, Probability: {confidence:.2f}\\n{decision}\"\n",
    "\n",
    "\n",
    "def prepare_image(img_pil):\n",
    "    img = img_pil.convert(\"L\")\n",
    "    img = img.resize((48, 48))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "def predict_emotion_face(image):\n",
    "    try:\n",
    "        processed_image = prepare_image(image)\n",
    "        prediction = facial_model.predict(processed_image)\n",
    "        predicted_index = np.argmax(prediction)\n",
    "        predicted_class = class_names[predicted_index]\n",
    "        confidence = round(float(prediction[0][predicted_index]) * 100, 2)\n",
    "\n",
    "        frustration_labels = [\"Angry\", \"Fear\", \"Sad\", \"Disgust\"]\n",
    "        if predicted_class in frustration_labels and confidence >= 70:\n",
    "            alert = \"🚨 Human Agent Required\"\n",
    "        else:\n",
    "            alert = \"✅ Bot Can Handle\"\n",
    "\n",
    "        return f\"Emotion: {predicted_class}, Probability: {confidence:.2f}%\", alert\n",
    "    except Exception as e:\n",
    "        return f\"Prediction Error: {str(e)}\", \"⚠️ Error\"\n",
    "\n",
    "def predict_emotion_speech(audio_path):\n",
    "    try:\n",
    "        speech, sr = librosa.load(audio_path, sr=16000)\n",
    "        inputs = speech_processor(speech, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "        with torch.no_grad():\n",
    "            logits = speech_model(**inputs).logits\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "        top_idx = int(np.argmax(probs))\n",
    "        top_emotion = id2label[top_idx]\n",
    "        top_prob = probs[top_idx] * 100\n",
    "\n",
    "        frustration_labels = [\"Anger\", \"Fear\", \"Sad\", \"Disgust\"]\n",
    "        if top_emotion in frustration_labels and top_prob >= 70:\n",
    "            alert = \"🚨 Human Agent Required\"\n",
    "        else:\n",
    "            alert = \"✅ Bot Can Handle\"\n",
    "\n",
    "        return f\"Emotion: {top_emotion}, Probability: {top_prob:.2f}%\", alert\n",
    "    except Exception as e:\n",
    "        return f\"Speech Error: {str(e)}\", \"⚠️ Error\"\n",
    "\n",
    "# Gradio UI\n",
    "with gr.Blocks(title=\"Multimodal Emotion Detection\") as app:\n",
    "    gr.Markdown(\"\"\"\n",
    "    <h1 style='text-align: center; color: #3B82F6;'>Multimodal Emotion Detection App</h1>\n",
    "    <p style='text-align: center;'>Detect emotions from text, face image, or speech to guide bot/human handover.</p>\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Tab(\"Text Emotion Detection\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                text_input = gr.Textbox(lines=3, label=\"Enter your text\")\n",
    "                gr.Examples([\n",
    "                    \"I'm really pissed off. Don’t make me repeat myself again.\",\n",
    "                    \"I’m tired and frustrated with your service.\",\n",
    "                    \"I’m very happy with the support!\",\n",
    "                    \"It’s okay, I guess.\",\n",
    "                    \"I feel neutral.\",\n",
    "                ], inputs=text_input)\n",
    "                text_button = gr.Button(\"Analyze Emotion\")\n",
    "            with gr.Column():\n",
    "                text_output = gr.Textbox(label=\"Predicted Emotion\")\n",
    "                text_alert = gr.Textbox(label=\"Agent Escalation\")\n",
    "        text_button.click(fn=predict_emotion_text, inputs=text_input, outputs=[text_output, text_alert])\n",
    "\n",
    "    with gr.Tab(\"Facial Emotion Detection\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                image_input = gr.Image(type=\"pil\", label=\"Upload a face image\")\n",
    "                image_button = gr.Button(\"Detect Emotion\")\n",
    "            with gr.Column():\n",
    "                image_output = gr.Textbox(label=\"Predicted Emotion\")\n",
    "                image_alert = gr.Textbox(label=\"Agent Escalation\")\n",
    "        image_button.click(fn=predict_emotion_face, inputs=image_input, outputs=[image_output, image_alert])\n",
    "\n",
    "    with gr.Tab(\"Speech Emotion Detection\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                audio_input = gr.Audio(type=\"filepath\", label=\"Upload Speech\")\n",
    "                audio_button = gr.Button(\"Detect Emotion\")\n",
    "            with gr.Column():\n",
    "                audio_output = gr.Textbox(label=\"Predicted Emotion\")\n",
    "                audio_alert = gr.Textbox(label=\"Agent Escalation\")\n",
    "        audio_button.click(fn=predict_emotion_speech, inputs=audio_input, outputs=[audio_output, audio_alert])\n",
    "\n",
    "app.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "CMk1troAcM5y",
    "outputId": "6add6e11-5de0-47b9-d0d4-a2c61823df28"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "import librosa\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from nltk.stem import PorterStemmer\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Load text model components\n",
    "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
    "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
    "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
    "\n",
    "# Load facial model\n",
    "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Load speech model\n",
    "model_name = \"prithivMLmods/Speech-Emotion-Classification\"\n",
    "speech_model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "id2label = {\n",
    "    0: \"Anger\", 1: \"Calm\", 2: \"Disgust\", 3: \"Fear\",\n",
    "    4: \"Happy\", 5: \"Neutral\", 6: \"Sad\", 7: \"Surprised\"\n",
    "}\n",
    "\n",
    "# ---------- TEXT EMOTION FUNCTIONS ----------\n",
    "def clean_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.lower().split()\n",
    "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def predict_text_emotion(input_text):\n",
    "    cleaned_text = clean_text(input_text)\n",
    "    input_vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
    "    predicted_label = lg.predict(input_vectorized)[0]\n",
    "    predicted_emotion = lb.inverse_transform([predicted_label])[0]\n",
    "    probability = np.max(lg.predict_proba(input_vectorized)) * 100\n",
    "    return predicted_emotion, round(probability, 2)\n",
    "\n",
    "# ---------- FACIAL EMOTION FUNCTIONS ----------\n",
    "def prepare_image(img_pil):\n",
    "    img = img_pil.convert(\"L\")\n",
    "    img = img.resize((48, 48))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "def detect_facial_emotion(image):\n",
    "    try:\n",
    "        processed_image = prepare_image(image)\n",
    "        prediction = facial_model.predict(processed_image)\n",
    "        predicted_index = np.argmax(prediction)\n",
    "        predicted_class = class_names[predicted_index]\n",
    "        confidence = float(prediction[0][predicted_index]) * 100\n",
    "        return predicted_class, round(confidence, 2)\n",
    "    except Exception as e:\n",
    "        return \"Error\", 0.0\n",
    "\n",
    "# ---------- SPEECH EMOTION FUNCTIONS ----------\n",
    "def detect_speech_emotion(audio_path):\n",
    "    try:\n",
    "        speech, sr = librosa.load(audio_path, sr=16000)\n",
    "        inputs = processor(speech, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "        with torch.no_grad():\n",
    "            logits = speech_model(**inputs).logits\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "        top_idx = int(np.argmax(probs))\n",
    "        top_label = id2label[top_idx]\n",
    "        top_confidence = float(probs[top_idx]) * 100\n",
    "        return top_label, round(top_confidence, 2)\n",
    "    except Exception as e:\n",
    "        return \"Error\", 0.0\n",
    "\n",
    "# ---------- ESCALATION LOGIC ----------\n",
    "def should_escalate(emotion, confidence):\n",
    "    escalate_emotions = ['Angry', 'Fear', 'Disgust', 'Frustrated']\n",
    "    return emotion in escalate_emotions and confidence >= 60\n",
    "\n",
    "# ---------- GRADIO UI ----------\n",
    "with gr.Blocks(title=\"Multimodal Customer Support Emotion Analyzer\") as app:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # 🧠 Multimodal Emotion Analyzer for Customer Support\n",
    "    Use **Text**, **Facial Image**, or **Speech** to detect customer emotion.\n",
    "    System decides if escalation to human agent is needed.\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### 📝 Text Input\")\n",
    "            text_input = gr.Textbox(lines=3, placeholder=\"Type something...\", label=\"Text\")\n",
    "            text_examples = gr.Examples(\n",
    "                examples=[\n",
    "                    \"I’m really pissed off. Don’t make me repeat myself again.\",\n",
    "                    \"Everything is broken again, seriously?\",\n",
    "                    \"Hi, I need help with my order.\",\n",
    "                    \"This is the best support ever!\"\n",
    "                ],\n",
    "                inputs=text_input\n",
    "            )\n",
    "            text_btn = gr.Button(\"Analyze Text\")\n",
    "            text_output = gr.Textbox(label=\"Text Emotion (Confidence%)\")\n",
    "\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### 📷 Facial Image Input\")\n",
    "            image_input = gr.Image(type=\"pil\", label=\"Upload Face Image\")\n",
    "            image_btn = gr.Button(\"Analyze Image\")\n",
    "            image_output = gr.Textbox(label=\"Facial Emotion (Confidence%)\")\n",
    "\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### 🎙️ Speech Input\")\n",
    "            audio_input = gr.Audio(type=\"filepath\", label=\"Upload Audio\")\n",
    "            audio_btn = gr.Button(\"Analyze Audio\")\n",
    "            audio_output = gr.Textbox(label=\"Speech Emotion (Confidence%)\")\n",
    "\n",
    "    final_decision = gr.Textbox(label=\"🔔 Escalation Decision\", interactive=False)\n",
    "\n",
    "    def fusion(text, image, audio):\n",
    "        results = []\n",
    "        if text:\n",
    "            emo, conf = predict_text_emotion(text)\n",
    "            results.append((emo, conf))\n",
    "            text_result = f\"{emo} ({conf}%)\"\n",
    "        else:\n",
    "            text_result = \"No text\"\n",
    "\n",
    "        if image:\n",
    "            emo, conf = detect_facial_emotion(image)\n",
    "            results.append((emo, conf))\n",
    "            image_result = f\"{emo} ({conf}%)\"\n",
    "        else:\n",
    "            image_result = \"No image\"\n",
    "\n",
    "        if audio:\n",
    "            emo, conf = detect_speech_emotion(audio)\n",
    "            results.append((emo, conf))\n",
    "            audio_result = f\"{emo} ({conf}%)\"\n",
    "        else:\n",
    "            audio_result = \"No audio\"\n",
    "\n",
    "        # Escalation logic based on any high-intensity negative emotion\n",
    "        needs_escalation = any(should_escalate(e, c) for e, c in results)\n",
    "        decision = \"🚨 Escalate to Human Agent\" if needs_escalation else \"✅ Bot can handle\"\n",
    "\n",
    "        return text_result, image_result, audio_result, decision\n",
    "\n",
    "    gr.Button(\"🔍 Final Multimodal Analysis\").click(\n",
    "        fn=fusion,\n",
    "        inputs=[text_input, image_input, audio_input],\n",
    "        outputs=[text_output, image_output, audio_output, final_decision]\n",
    "    )\n",
    "\n",
    "app.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7RdUWj1dKC3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "id": "9UVwSmX4dKco",
    "outputId": "2d222280-247c-4a14-f70a-8e94858e2d25"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "import torch\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from nltk.stem import PorterStemmer\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "\n",
    "# ------------------- SETUP -------------------\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Text model components\n",
    "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
    "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
    "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
    "\n",
    "# Facial model\n",
    "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Speech model\n",
    "speech_model_name = \"prithivMLmods/Speech-Emotion-Classification\"\n",
    "speech_model = Wav2Vec2ForSequenceClassification.from_pretrained(speech_model_name)\n",
    "speech_processor = Wav2Vec2FeatureExtractor.from_pretrained(speech_model_name)\n",
    "speech_id2label = {\n",
    "    0: \"Anger\", 1: \"Calm\", 2: \"Disgust\", 3: \"Fear\",\n",
    "    4: \"Happy\", 5: \"Neutral\", 6: \"Sad\", 7: \"Surprised\"\n",
    "}\n",
    "\n",
    "# ------------------- TEXT -------------------\n",
    "def clean_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.lower().split()\n",
    "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def predict_text_emotion(text):\n",
    "    if not text.strip():\n",
    "        return None, None\n",
    "    cleaned_text = clean_text(text)\n",
    "    vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
    "    label = lg.predict(vectorized)[0]\n",
    "    prob = np.max(lg.predict_proba(vectorized))\n",
    "    emotion = lb.inverse_transform([label])[0]\n",
    "    return emotion, round(prob * 100, 2)\n",
    "\n",
    "# ------------------- FACE -------------------\n",
    "def prepare_image(img_pil):\n",
    "    img = img_pil.convert(\"L\").resize((48, 48))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    return np.expand_dims(img_array, axis=0)\n",
    "\n",
    "def predict_face_emotion(image):\n",
    "    if image is None:\n",
    "        return None, None\n",
    "    processed = prepare_image(image)\n",
    "    pred = facial_model.predict(processed)\n",
    "    idx = np.argmax(pred)\n",
    "    return class_names[idx], round(float(pred[0][idx]) * 100, 2)\n",
    "\n",
    "# ------------------- SPEECH -------------------\n",
    "def predict_speech_emotion(audio_path):\n",
    "    if audio_path is None:\n",
    "        return None, None\n",
    "    speech, sr = librosa.load(audio_path, sr=16000)\n",
    "    inputs = speech_processor(speech, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        logits = speech_model(**inputs).logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "    top_idx = int(np.argmax(probs))\n",
    "    return speech_id2label[top_idx], round(probs[top_idx] * 100, 2)\n",
    "\n",
    "# ------------------- FUSION -------------------\n",
    "def escalation_needed(emotion_conf_list):\n",
    "    \"\"\"Return True if any emotion is Anger, Fear, Disgust with ≥ 40% confidence\"\"\"\n",
    "    for emotion, confidence in emotion_conf_list:\n",
    "        if emotion in ['Angry', 'Fear', 'Disgust', 'Anger'] and confidence is not None and confidence >= 40:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# ------------------- UI -------------------\n",
    "with gr.Blocks(title=\"Customer Support Escalation System\") as app:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # 🤖 Customer Emotion Monitoring System\n",
    "    Upload or provide any modality (Text / Facial Image / Audio) and the system will detect emotion and decide whether a bot can handle it or escalation to a human is needed.\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        text_input = gr.Textbox(label=\"Text Input\", lines=3, placeholder=\"e.g., I'm really frustrated with this service!\")\n",
    "        image_input = gr.Image(type=\"pil\", label=\"Upload Face Image\")\n",
    "        audio_input = gr.Audio(type=\"filepath\", label=\"Upload Audio File\")\n",
    "\n",
    "    with gr.Row():\n",
    "        analyze_button = gr.Button(\"Analyze Customer Emotion\")\n",
    "\n",
    "    with gr.Row():\n",
    "        text_out = gr.Textbox(label=\"Text Emotion Result\")\n",
    "        face_out = gr.Textbox(label=\"Facial Emotion Result\")\n",
    "        audio_out = gr.Textbox(label=\"Speech Emotion Result\")\n",
    "\n",
    "    final_decision = gr.Textbox(label=\"System Decision: Escalate to Human or Bot Can Handle\", lines=2)\n",
    "\n",
    "    def analyze_all(text, image, audio):\n",
    "        text_emotion, text_conf = predict_text_emotion(text)\n",
    "        face_emotion, face_conf = predict_face_emotion(image)\n",
    "        speech_emotion, speech_conf = predict_speech_emotion(audio)\n",
    "\n",
    "        text_result = f\"{text_emotion} ({text_conf}%)\" if text_emotion else \"No text input\"\n",
    "        face_result = f\"{face_emotion} ({face_conf}%)\" if face_emotion else \"No image input\"\n",
    "        speech_result = f\"{speech_emotion} ({speech_conf}%)\" if speech_emotion else \"No audio input\"\n",
    "\n",
    "        emotions = [(text_emotion, text_conf), (face_emotion, face_conf), (speech_emotion, speech_conf)]\n",
    "\n",
    "        decision = \"🔴 Escalate to Human Agent\" if escalation_needed(emotions) else \"🟢 Bot Can Handle\"\n",
    "\n",
    "        return text_result, face_result, speech_result, decision\n",
    "\n",
    "    analyze_button.click(\n",
    "        fn=analyze_all,\n",
    "        inputs=[text_input, image_input, audio_input],\n",
    "        outputs=[text_out, face_out, audio_out, final_decision]\n",
    "    )\n",
    "\n",
    "app.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "7a01d7dcb3df45bb96f0cfb817447856",
      "5e5eb0efff554a4d8688cf75c26ed975",
      "9c9626d63a5746b49edc6f13b8a11b12",
      "3b2f4687744a48f3ab3d6703cf678125",
      "c5fe4ad7622a44d3a10402c0f15033fb",
      "a0c349cf779046289ceb4090a79e303c",
      "287a46b2b3724c44a63b342ac3439de6",
      "f8a0fccd8acc46dab0a2929fb4848591",
      "f5bc68aaef0e4a50802e37a35f7f09ff",
      "40d7340f390f4fd788d6f4cdf22c429e",
      "7d9d6891aa81410faf7474889af514bc",
      "e10804ff5b294e93bc2d1bf89110f70e",
      "877c620806c84430b17b15ae3345dd7c",
      "c59af67155114543850df7388b07e976",
      "6ffe6b6bf72d41408a76460747294e42",
      "ec5976c314e041a391dd771f456ec1da",
      "6d1c9e5261d843ed8d9427c7ace874cd",
      "46c0e61de6ea42e5a7928de588ab5743",
      "7f69a43ff8c94ae290acaa3f4c8a13c4",
      "c83eabfb31064ab28101916426a138d2",
      "a9eb450a4cef48de9d34534e91fb014c",
      "3598e3d8ea3c439ebe556ad11cfc8293",
      "9dc15d1c8bd3428db66b3381dffc0549",
      "8fb34107b1154664bffed38d38a4e252",
      "aa031d43070841c8a3c270efbe4c871e",
      "e77193672cc040e8a762267c68dc036b",
      "ed1bae75803a4f28bc912c5b0d9f9cca",
      "c1bb2149902c44c6be6a3a980e5aaa36",
      "f002913927f84529a06050c635b91867",
      "6c3e1a7607884cecba5c4a48ddc8d3be",
      "03acfabcfb3e418faca6ecdf52cd2405",
      "5c27250555bc4bea9d565edf3331656a",
      "36fe162e32c7464f8e32c58d2af2b452"
     ]
    },
    "id": "Ye_PCLLwgE7r",
    "outputId": "c23aa909-697f-4362-cafc-0fdc22464d97"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "import torch\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from nltk.stem import PorterStemmer\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "\n",
    "# --- Download NLTK stopwords ---\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# --- Load text model components ---\n",
    "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
    "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
    "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
    "\n",
    "# --- Load facial emotion model ---\n",
    "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# --- Load speech model ---\n",
    "speech_model_name = \"prithivMLmods/Speech-Emotion-Classification\"\n",
    "speech_model = Wav2Vec2ForSequenceClassification.from_pretrained(speech_model_name)\n",
    "speech_processor = Wav2Vec2FeatureExtractor.from_pretrained(speech_model_name)\n",
    "speech_id2label = {\n",
    "    0: \"Anger\", 1: \"Calm\", 2: \"Disgust\", 3: \"Fear\",\n",
    "    4: \"Happy\", 5: \"Neutral\", 6: \"Sad\", 7: \"Surprised\"\n",
    "}\n",
    "\n",
    "# ---------- TEXT EMOTION FUNCTIONS ----------\n",
    "def clean_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.lower().split()\n",
    "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def predict_text_emotion(input_text):\n",
    "    cleaned_text = clean_text(input_text)\n",
    "    input_vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
    "    predicted_label = lg.predict(input_vectorized)[0]\n",
    "    predicted_emotion = lb.inverse_transform([predicted_label])[0]\n",
    "    probability = np.max(lg.predict_proba(input_vectorized)) * 100\n",
    "    return f\"{predicted_emotion} ({probability:.2f}%)\"\n",
    "\n",
    "# ---------- FACIAL EMOTION FUNCTIONS ----------\n",
    "def prepare_image(img_pil):\n",
    "    img = img_pil.convert(\"L\")\n",
    "    img = img.resize((48, 48))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "def predict_facial_emotion(image):\n",
    "    try:\n",
    "        processed_image = prepare_image(image)\n",
    "        prediction = facial_model.predict(processed_image)\n",
    "        predicted_index = np.argmax(prediction)\n",
    "        predicted_class = class_names[predicted_index]\n",
    "        confidence = float(prediction[0][predicted_index]) * 100\n",
    "        return f\"{predicted_class} ({confidence:.2f}%)\"\n",
    "    except Exception as e:\n",
    "        return f\"Prediction Error: {str(e)}\"\n",
    "\n",
    "# ---------- SPEECH EMOTION FUNCTION ----------\n",
    "def predict_speech_emotion(audio_path):\n",
    "    try:\n",
    "        speech, sr = librosa.load(audio_path, sr=16000)\n",
    "        inputs = speech_processor(speech, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "        with torch.no_grad():\n",
    "            logits = speech_model(**inputs).logits\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "        top_idx = int(np.argmax(probs))\n",
    "        top_label = speech_id2label[top_idx]\n",
    "        top_prob = float(probs[top_idx]) * 100\n",
    "        return f\"{top_label} ({top_prob:.2f}%)\"\n",
    "    except Exception as e:\n",
    "        return f\"Audio Error: {str(e)}\"\n",
    "\n",
    "# ---------- ESCALATION LOGIC ----------\n",
    "def should_escalate(results):\n",
    "    escalation_emotions = {\"anger\", \"fear\", \"disgust\"}\n",
    "    threshold = 40.0\n",
    "\n",
    "    for result in results:\n",
    "        if result:\n",
    "            try:\n",
    "                emotion, conf = result.split(\"(\")\n",
    "                emotion = emotion.strip().lower()\n",
    "                confidence = float(conf.strip().replace(\"%)\", \"\").replace(\")\", \"\"))\n",
    "                if emotion in escalation_emotions and confidence >= threshold:\n",
    "                    return True\n",
    "            except Exception as e:\n",
    "                print(\"Error parsing result:\", result, \"| Error:\", e)\n",
    "                continue\n",
    "    return False\n",
    "\n",
    "# ---------- FINAL FUSION LOGIC ----------\n",
    "def fused_decision(text_input, image_input, audio_input):\n",
    "    text_result = predict_text_emotion(text_input) if text_input else None\n",
    "    image_result = predict_facial_emotion(image_input) if image_input else None\n",
    "    audio_result = predict_speech_emotion(audio_input) if audio_input else None\n",
    "\n",
    "    should_alert = should_escalate([text_result, image_result, audio_result])\n",
    "    final_decision = \"🔴 Escalate to human agent\" if should_alert else \"🟢 Bot can handle\"\n",
    "\n",
    "    return text_result or \"No Text Input\", image_result or \"No Image Input\", audio_result or \"No Audio Input\", final_decision\n",
    "\n",
    "# ---------- UI ----------\n",
    "with gr.Blocks(title=\"Customer Support Emotion Escalation System\") as app:\n",
    "    gr.Markdown(\"\"\"\n",
    "    <h1 style='text-align: center; color: #3B82F6;'>🤖 Multimodal Emotion Detection & Escalation</h1>\n",
    "    <p style='text-align: center;'>Analyze customer's emotion via Text, Facial Image, or Voice — and decide if human support is needed.</p>\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            text_input = gr.Textbox(label=\"Text Input\", placeholder=\"Type something the customer said...\")\n",
    "            gr.Examples(\n",
    "                examples=[\n",
    "                    \"I'm really pissed off right now!\",\n",
    "                    \"Why is your app so buggy and slow?\",\n",
    "                    \"This is absolutely fine, thank you!\"\n",
    "                ],\n",
    "                inputs=text_input\n",
    "            )\n",
    "\n",
    "            image_input = gr.Image(type=\"pil\", label=\"Facial Image Input\")\n",
    "            audio_input = gr.Audio(type=\"filepath\", label=\"Speech Audio Input\")\n",
    "            btn = gr.Button(\"🔍 Analyze Emotion\")\n",
    "\n",
    "        with gr.Column():\n",
    "            text_output = gr.Textbox(label=\"Text Emotion\")\n",
    "            image_output = gr.Textbox(label=\"Facial Emotion\")\n",
    "            audio_output = gr.Textbox(label=\"Speech Emotion\")\n",
    "            final_decision = gr.Textbox(label=\"System Decision (Bot or Human)\", lines=1)\n",
    "\n",
    "    btn.click(fn=fused_decision,\n",
    "              inputs=[text_input, image_input, audio_input],\n",
    "              outputs=[text_output, image_output, audio_output, final_decision])\n",
    "\n",
    "app.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VU6pA6z7uROV"
   },
   "source": [
    "# Webcam enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "zuLrTgVnuVpk",
    "outputId": "cce47474-18fb-44a8-e793-a1933f114a16"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Load text model components\n",
    "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
    "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
    "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
    "\n",
    "# Load facial model\n",
    "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# ---------- TEXT EMOTION FUNCTIONS ----------\n",
    "def clean_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.lower().split()\n",
    "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def predict_emotion(input_text):\n",
    "    cleaned_text = clean_text(input_text)\n",
    "    input_vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
    "    predicted_label = lg.predict(input_vectorized)[0]\n",
    "    predicted_emotion = lb.inverse_transform([predicted_label])[0]\n",
    "    probability = np.max(lg.predict_proba(input_vectorized))\n",
    "    return f\"Emotion: {predicted_emotion}, Probability: {probability:.2f}\"\n",
    "\n",
    "# ---------- FACIAL EMOTION FUNCTIONS ----------\n",
    "def prepare_image(img_pil):\n",
    "    img = img_pil.convert(\"L\")  # Grayscale\n",
    "    img = img.resize((48, 48))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "def detect_emotion(image):\n",
    "    if image is None:\n",
    "        return \"No image captured. Please try again.\"\n",
    "    try:\n",
    "        processed_image = prepare_image(image)\n",
    "        prediction = facial_model.predict(processed_image)\n",
    "        predicted_index = np.argmax(prediction)\n",
    "        predicted_class = class_names[predicted_index]\n",
    "        confidence = round(float(prediction[0][predicted_index]) * 100, 2)\n",
    "        return f\"{predicted_class} ({confidence}%)\"\n",
    "    except Exception as e:\n",
    "        return f\"Prediction Error: {str(e)}\"\n",
    "\n",
    "# ---------- GRADIO UI ----------\n",
    "with gr.Blocks(title=\"Emotion Detection Suite\") as app:\n",
    "    gr.Markdown(\n",
    "        \"<h1 style='text-align: center; color: #3B82F6;'>Multimodal Emotion Detection App</h1>\"\n",
    "        \"<p style='text-align: center;'>Detect emotions from text and facial expressions</p>\"\n",
    "    )\n",
    "\n",
    "    with gr.Tab(\"Text Emotion Detection\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                text_input = gr.Textbox(lines=3, placeholder=\"Type a sentence...\", label=\"Enter your text\")\n",
    "                text_examples = gr.Examples(\n",
    "                    examples=[\n",
    "                        \"I am feeling fantastic today!\",\n",
    "                        \"Why does everything feel so stressful?\",\n",
    "                        \"I'm scared about tomorrow's interview.\",\n",
    "                        \"This makes me so angry!\",\n",
    "                        \"What a pleasant surprise!\",\n",
    "                        \"I’m just feeling neutral right now.\"\n",
    "                    ],\n",
    "                    inputs=text_input\n",
    "                )\n",
    "                text_button = gr.Button(\"Analyze Emotion\")\n",
    "            with gr.Column():\n",
    "                text_output = gr.Textbox(label=\"Predicted Emotion\")\n",
    "\n",
    "        text_button.click(fn=predict_emotion, inputs=text_input, outputs=text_output)\n",
    "\n",
    "    with gr.Tab(\"Facial Emotion Detection\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "               image_input = gr.Image(source=\"webcam\", tool=\"editor\", type=\"pil\", label=\"Capture your face using webcam\")\n",
    "                image_button = gr.Button(\"Detect Emotion\")\n",
    "            with gr.Column():\n",
    "                image_output = gr.Textbox(label=\"Predicted Emotion\")\n",
    "\n",
    "        image_button.click(fn=detect_emotion, inputs=image_input, outputs=image_output)\n",
    "\n",
    "# Use share=True in Colab or for public access\n",
    "app.launch(share=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQ-TPjYpxmQl"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "gb_muNdnxsGk",
    "outputId": "d4da6df8-bef9-4129-90ff-3ef707778cef"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Load text model components\n",
    "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
    "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
    "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
    "\n",
    "# Load facial model\n",
    "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "\n",
    "# ---------- TEXT EMOTION FUNCTIONS ----------\n",
    "def clean_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.lower().split()\n",
    "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def predict_emotion(input_text):\n",
    "    cleaned_text = clean_text(input_text)\n",
    "    input_vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
    "    predicted_label = lg.predict(input_vectorized)[0]\n",
    "    predicted_emotion = lb.inverse_transform([predicted_label])[0]\n",
    "    probability = np.max(lg.predict_proba(input_vectorized))\n",
    "    return f\"Emotion: {predicted_emotion}, Probability: {probability:.2f}\"\n",
    "\n",
    "\n",
    "# ---------- FACIAL EMOTION FUNCTIONS ----------\n",
    "def prepare_image(img_pil):\n",
    "    img = img_pil.convert(\"L\")\n",
    "    img = img.resize((48, 48))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "def detect_emotion(image):\n",
    "    try:\n",
    "        processed_image = prepare_image(image)\n",
    "        prediction = facial_model.predict(processed_image)\n",
    "        predicted_index = np.argmax(prediction)\n",
    "        predicted_class = class_names[predicted_index]\n",
    "        confidence = round(float(prediction[0][predicted_index]) * 100, 2)\n",
    "        return f\"{predicted_class} ({confidence}%)\"\n",
    "    except Exception as e:\n",
    "        return f\"Prediction Error: {str(e)}\"\n",
    "\n",
    "\n",
    "# ---------- GRADIO UI ----------\n",
    "with gr.Blocks(title=\"Emotion Detection Suite\") as app:\n",
    "    gr.Markdown(\n",
    "        \"<h1 style='text-align: center; color: #3B82F6;'>Multimodal Emotion Detection App</h1>\"\n",
    "        \"<p style='text-align: center;'>Detect emotions from text and facial expressions</p>\"\n",
    "    )\n",
    "\n",
    "    with gr.Tab(\"Text Emotion Detection\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                text_input = gr.Textbox(lines=3, placeholder=\"Type a sentence...\", label=\"Enter your text\")\n",
    "                text_examples = gr.Examples(\n",
    "                    examples=[\n",
    "                        \"I am feeling fantastic today!\",\n",
    "                        \"Why does everything feel so stressful?\",\n",
    "                        \"I'm scared about tomorrow's interview.\",\n",
    "                        \"This makes me so angry!\",\n",
    "                        \"What a pleasant surprise!\",\n",
    "                        \"I’m just feeling neutral right now.\"\n",
    "                    ],\n",
    "                    inputs=text_input\n",
    "                )\n",
    "                text_button = gr.Button(\"Analyze Emotion\")\n",
    "            with gr.Column():\n",
    "                text_output = gr.Textbox(label=\"Predicted Emotion\")\n",
    "\n",
    "        text_button.click(fn=predict_emotion, inputs=text_input, outputs=text_output)\n",
    "\n",
    "with gr.Tab(\"Facial Emotion Detection\"):\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "           image_input = gr.Image(type=\"pil\", tool=\"editor\", label=\"Take a photo (click camera icon)\")\n",
    "\n",
    "           image_button = gr.Button(\"Detect Emotion\")\n",
    "        with gr.Column():\n",
    "            image_output = gr.Textbox(label=\"Predicted Emotion\")\n",
    "\n",
    "    image_button.click(fn=detect_emotion, inputs=image_input, outputs=image_output)\n",
    "\n",
    "app.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QfRwF7N7ySpH",
    "outputId": "0b444085-6e0f-472f-e94a-4d4b7c4b4eb6"
   },
   "outputs": [],
   "source": [
    "# Install required libraries if not already installed\n",
    "!pip install --upgrade gradio numpy nltk tensorflow pillow\n",
    "\n",
    "# --- Imports ---\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# --- Download NLTK Stopwords ---\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# --- Load Text Emotion Model Components ---\n",
    "lg = pickle.load(open('logistic_regresion.pkl', 'rb'))\n",
    "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
    "lb = pickle.load(open('label_encoder.pkl', 'rb'))\n",
    "\n",
    "# --- Load Facial Emotion Detection Model ---\n",
    "facial_model = load_model(\"/content/drive/MyDrive/facial_emotion_detection_model.h5\")\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "\n",
    "# ---------------- TEXT EMOTION FUNCTIONS ----------------\n",
    "def clean_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.lower().split()\n",
    "    text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def predict_emotion(input_text):\n",
    "    cleaned_text = clean_text(input_text)\n",
    "    input_vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
    "    predicted_label = lg.predict(input_vectorized)[0]\n",
    "    predicted_emotion = lb.inverse_transform([predicted_label])[0]\n",
    "    probability = np.max(lg.predict_proba(input_vectorized))\n",
    "    return f\"Emotion: {predicted_emotion}, Probability: {probability:.2f}\"\n",
    "\n",
    "\n",
    "# ---------------- FACIAL EMOTION FUNCTIONS ----------------\n",
    "def prepare_image(img_pil):\n",
    "    img = img_pil.convert(\"L\")\n",
    "    img = img.resize((48, 48))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "def detect_emotion(image):\n",
    "    try:\n",
    "        processed_image = prepare_image(image)\n",
    "        prediction = facial_model.predict(processed_image)\n",
    "        predicted_index = np.argmax(prediction)\n",
    "        predicted_class = class_names[predicted_index]\n",
    "        confidence = round(float(prediction[0][predicted_index]) * 100, 2)\n",
    "        return f\"{predicted_class} ({confidence}%)\"\n",
    "    except Exception as e:\n",
    "        return f\"Prediction Error: {str(e)}\"\n",
    "\n",
    "\n",
    "# ---------------- GRADIO INTERFACE ----------------\n",
    "with gr.Blocks(title=\"Multimodal Emotion Detection\") as app:\n",
    "    gr.Markdown(\n",
    "        \"<h1 style='text-align: center; color: #4F46E5;'>🎭 Multimodal Emotion Detection App</h1>\"\n",
    "        \"<p style='text-align: center;'>Detect human emotions using Text or Facial expressions</p>\"\n",
    "    )\n",
    "\n",
    "    with gr.Tab(\"📝 Text Emotion Detection\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                text_input = gr.Textbox(lines=3, placeholder=\"Type a sentence...\", label=\"Enter your text\")\n",
    "                text_examples = gr.Examples(\n",
    "                    examples=[\n",
    "                        \"I am feeling fantastic today!\",\n",
    "                        \"Why does everything feel so stressful?\",\n",
    "                        \"I'm scared about tomorrow's interview.\",\n",
    "                        \"This makes me so angry!\",\n",
    "                        \"What a pleasant surprise!\",\n",
    "                        \"I’m just feeling neutral right now.\"\n",
    "                    ],\n",
    "                    inputs=text_input\n",
    "                )\n",
    "                text_button = gr.Button(\"Analyze Emotion\")\n",
    "            with gr.Column():\n",
    "                text_output = gr.Textbox(label=\"Predicted Emotion\")\n",
    "        text_button.click(fn=predict_emotion, inputs=text_input, outputs=text_output)\n",
    "\n",
    "    with gr.Tab(\"📷 Facial Emotion Detection\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                image_input = gr.Image(source=\"webcam\", type=\"pil\", label=\"Take a photo or upload an image\")\n",
    "                image_button = gr.Button(\"Detect Emotion\")\n",
    "            with gr.Column():\n",
    "                image_output = gr.Textbox(label=\"Predicted Emotion\")\n",
    "        image_button.click(fn=detect_emotion, inputs=image_input, outputs=image_output)\n",
    "\n",
    "app.launch()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03acfabcfb3e418faca6ecdf52cd2405": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "166502e8de944d21ba4248042762790c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54395a0210204c8cbfa1e062807b9897",
      "placeholder": "​",
      "style": "IPY_MODEL_8f4d4f201fa94d43bbb8099807a10b16",
      "value": " 378M/378M [00:18&lt;00:00, 17.0MB/s]"
     }
    },
    "16cf59a900644f15a25ec3f31c793c5a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "287a46b2b3724c44a63b342ac3439de6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2cef7670d08645d4ae5e968f7979bad7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c40711c0e1e548389bbca800035f095e",
      "placeholder": "​",
      "style": "IPY_MODEL_fe682990628048649d28a58eeb05f5ef",
      "value": "preprocessor_config.json: 100%"
     }
    },
    "30325925153c4a5fae6afb53f9c5f4f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3598e3d8ea3c439ebe556ad11cfc8293": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "36fe162e32c7464f8e32c58d2af2b452": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b2f4687744a48f3ab3d6703cf678125": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40d7340f390f4fd788d6f4cdf22c429e",
      "placeholder": "​",
      "style": "IPY_MODEL_7d9d6891aa81410faf7474889af514bc",
      "value": " 2.62k/? [00:00&lt;00:00, 158kB/s]"
     }
    },
    "3cd1d603be844446b65c549421357d27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3ea8f5ba98e34b329895ee401281bb3a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40d7340f390f4fd788d6f4cdf22c429e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42424cb0d84e48b1937a82df3ad8b422": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4260b33d7e9843618950a3bace7f1038": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "432e5436df3c4f5d929798ad95ff6262": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46bd1b82f91b4e4da93817fd1fb9b48e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9bd330c063143638e3f54690df858b8",
      "max": 215,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ccab155a0c3846b18d1a34fdbe0db20a",
      "value": 215
     }
    },
    "46c0e61de6ea42e5a7928de588ab5743": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "488ed5e6c0f64180a5855288dce1bd79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "49689d00a654443ab5d355e757cf76b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "509879ea62a84809aa05266c87c71a89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3710531e49c46e6b5c5f4e46c6c4889",
      "placeholder": "​",
      "style": "IPY_MODEL_92c52d4a7caa48a2a814dfd0c0a79b27",
      "value": " 2.62k/? [00:00&lt;00:00, 237kB/s]"
     }
    },
    "54395a0210204c8cbfa1e062807b9897": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "545a6d83c7484b72a4f7bb5c85a24a64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55da60e5b3c94fa1905589d070935d56",
      "placeholder": "​",
      "style": "IPY_MODEL_b80c6fc6ffff4723a865139e433d9fc3",
      "value": " 215/215 [00:00&lt;00:00, 9.36kB/s]"
     }
    },
    "55da60e5b3c94fa1905589d070935d56": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56736bb3ebf44eb38225f2b8e5a068fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ceb167c4927541b7908b4dd4929ca09f",
      "placeholder": "​",
      "style": "IPY_MODEL_abc8b4e8f1eb41ecbf2168fd14e01301",
      "value": "config.json: "
     }
    },
    "5c27250555bc4bea9d565edf3331656a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e5eb0efff554a4d8688cf75c26ed975": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0c349cf779046289ceb4090a79e303c",
      "placeholder": "​",
      "style": "IPY_MODEL_287a46b2b3724c44a63b342ac3439de6",
      "value": "config.json: "
     }
    },
    "60f9e63a0d3c43ca806a3cc1eee09d53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62109e980a3b4f708c710669d897adcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6c3e1a7607884cecba5c4a48ddc8d3be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d1c9e5261d843ed8d9427c7ace874cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ffe6b6bf72d41408a76460747294e42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a9eb450a4cef48de9d34534e91fb014c",
      "placeholder": "​",
      "style": "IPY_MODEL_3598e3d8ea3c439ebe556ad11cfc8293",
      "value": " 378M/378M [00:05&lt;00:00, 125MB/s]"
     }
    },
    "70255892a7b24d668d58ed0c0c63c2f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2cef7670d08645d4ae5e968f7979bad7",
       "IPY_MODEL_e3fd240723aa4328b8846577891cb69e",
       "IPY_MODEL_ad8a48da34c24fb5abf1ac6e7e76928b"
      ],
      "layout": "IPY_MODEL_987fce1bdad84aa8b441ab51123ca867"
     }
    },
    "70fca7f28301467d860dc6cf4617ad23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_56736bb3ebf44eb38225f2b8e5a068fc",
       "IPY_MODEL_d7cbcffd188844f393ebb7536dd021d2",
       "IPY_MODEL_86b70bbda97940ca86706f38f4499c0b"
      ],
      "layout": "IPY_MODEL_16cf59a900644f15a25ec3f31c793c5a"
     }
    },
    "772765d1a82c49ca91d898cc02c52222": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a56fcb59fc584bd4964d630a0ed0ca1e",
       "IPY_MODEL_d1dcebd86ec34e3caae6ed8eb1b466a3",
       "IPY_MODEL_166502e8de944d21ba4248042762790c"
      ],
      "layout": "IPY_MODEL_779e86ea272c46c9822918b038403861"
     }
    },
    "779e86ea272c46c9822918b038403861": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a01d7dcb3df45bb96f0cfb817447856": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5e5eb0efff554a4d8688cf75c26ed975",
       "IPY_MODEL_9c9626d63a5746b49edc6f13b8a11b12",
       "IPY_MODEL_3b2f4687744a48f3ab3d6703cf678125"
      ],
      "layout": "IPY_MODEL_c5fe4ad7622a44d3a10402c0f15033fb"
     }
    },
    "7d9d6891aa81410faf7474889af514bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7f69a43ff8c94ae290acaa3f4c8a13c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82c2dde1351c429ab86e09fe0de639e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "849c506eb9b04dbd8ada43b09f357fae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9dd41c5bb4e3402d8902cafe107c2ce7",
       "IPY_MODEL_89a2a334a2b44abbb10a736d994b4289",
       "IPY_MODEL_509879ea62a84809aa05266c87c71a89"
      ],
      "layout": "IPY_MODEL_aa71764071bf474099b5a749c746b5e7"
     }
    },
    "86b70bbda97940ca86706f38f4499c0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ea8f5ba98e34b329895ee401281bb3a",
      "placeholder": "​",
      "style": "IPY_MODEL_bb16d0d58f8641db8cc0bc99ad3fb858",
      "value": " 2.28k/? [00:00&lt;00:00, 227kB/s]"
     }
    },
    "877c620806c84430b17b15ae3345dd7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d1c9e5261d843ed8d9427c7ace874cd",
      "placeholder": "​",
      "style": "IPY_MODEL_46c0e61de6ea42e5a7928de588ab5743",
      "value": "model.safetensors: 100%"
     }
    },
    "89a2a334a2b44abbb10a736d994b4289": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1567e38a6b54460b70dd6bd0a214752",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3cd1d603be844446b65c549421357d27",
      "value": 1
     }
    },
    "8f1e49e7e0ba4d8f9fe47b735e31432f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f4d4f201fa94d43bbb8099807a10b16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8fb34107b1154664bffed38d38a4e252": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1bb2149902c44c6be6a3a980e5aaa36",
      "placeholder": "​",
      "style": "IPY_MODEL_f002913927f84529a06050c635b91867",
      "value": "preprocessor_config.json: 100%"
     }
    },
    "92c52d4a7caa48a2a814dfd0c0a79b27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "96cfac83b0bc4043829e0762ba863816": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fa4293c74ab14fdaad192c5eea6e305a",
       "IPY_MODEL_46bd1b82f91b4e4da93817fd1fb9b48e",
       "IPY_MODEL_545a6d83c7484b72a4f7bb5c85a24a64"
      ],
      "layout": "IPY_MODEL_8f1e49e7e0ba4d8f9fe47b735e31432f"
     }
    },
    "987fce1bdad84aa8b441ab51123ca867": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c9626d63a5746b49edc6f13b8a11b12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8a0fccd8acc46dab0a2929fb4848591",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f5bc68aaef0e4a50802e37a35f7f09ff",
      "value": 1
     }
    },
    "9dc15d1c8bd3428db66b3381dffc0549": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8fb34107b1154664bffed38d38a4e252",
       "IPY_MODEL_aa031d43070841c8a3c270efbe4c871e",
       "IPY_MODEL_e77193672cc040e8a762267c68dc036b"
      ],
      "layout": "IPY_MODEL_ed1bae75803a4f28bc912c5b0d9f9cca"
     }
    },
    "9dd41c5bb4e3402d8902cafe107c2ce7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a02929f19d6545a9a0b44269f7218343",
      "placeholder": "​",
      "style": "IPY_MODEL_42424cb0d84e48b1937a82df3ad8b422",
      "value": "config.json: "
     }
    },
    "a02929f19d6545a9a0b44269f7218343": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0c349cf779046289ceb4090a79e303c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a56fcb59fc584bd4964d630a0ed0ca1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c08fbefe13254c05929993186cd27135",
      "placeholder": "​",
      "style": "IPY_MODEL_82c2dde1351c429ab86e09fe0de639e9",
      "value": "model.safetensors: 100%"
     }
    },
    "a9eb450a4cef48de9d34534e91fb014c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa031d43070841c8a3c270efbe4c871e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c3e1a7607884cecba5c4a48ddc8d3be",
      "max": 215,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_03acfabcfb3e418faca6ecdf52cd2405",
      "value": 215
     }
    },
    "aa71764071bf474099b5a749c746b5e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abc8b4e8f1eb41ecbf2168fd14e01301": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad0cf33e9ab3432184ac2c53bc0d1213": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad8a48da34c24fb5abf1ac6e7e76928b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_432e5436df3c4f5d929798ad95ff6262",
      "placeholder": "​",
      "style": "IPY_MODEL_ad0cf33e9ab3432184ac2c53bc0d1213",
      "value": " 214/214 [00:00&lt;00:00, 18.6kB/s]"
     }
    },
    "af34c0f5e44f4c339d22d0b7466fe2ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b80c6fc6ffff4723a865139e433d9fc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bb16d0d58f8641db8cc0bc99ad3fb858": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c08fbefe13254c05929993186cd27135": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1bb2149902c44c6be6a3a980e5aaa36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3710531e49c46e6b5c5f4e46c6c4889": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c40711c0e1e548389bbca800035f095e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c59af67155114543850df7388b07e976": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f69a43ff8c94ae290acaa3f4c8a13c4",
      "max": 378308536,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c83eabfb31064ab28101916426a138d2",
      "value": 378308536
     }
    },
    "c5fe4ad7622a44d3a10402c0f15033fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c83eabfb31064ab28101916426a138d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ccab155a0c3846b18d1a34fdbe0db20a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ceb167c4927541b7908b4dd4929ca09f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1dcebd86ec34e3caae6ed8eb1b466a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60f9e63a0d3c43ca806a3cc1eee09d53",
      "max": 378308536,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_488ed5e6c0f64180a5855288dce1bd79",
      "value": 378308536
     }
    },
    "d7cbcffd188844f393ebb7536dd021d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49689d00a654443ab5d355e757cf76b1",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4260b33d7e9843618950a3bace7f1038",
      "value": 1
     }
    },
    "d9bd330c063143638e3f54690df858b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e10804ff5b294e93bc2d1bf89110f70e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_877c620806c84430b17b15ae3345dd7c",
       "IPY_MODEL_c59af67155114543850df7388b07e976",
       "IPY_MODEL_6ffe6b6bf72d41408a76460747294e42"
      ],
      "layout": "IPY_MODEL_ec5976c314e041a391dd771f456ec1da"
     }
    },
    "e1100dcfc56e41baa634cd50a69ece62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1567e38a6b54460b70dd6bd0a214752": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "e3fd240723aa4328b8846577891cb69e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30325925153c4a5fae6afb53f9c5f4f0",
      "max": 214,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_af34c0f5e44f4c339d22d0b7466fe2ba",
      "value": 214
     }
    },
    "e77193672cc040e8a762267c68dc036b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c27250555bc4bea9d565edf3331656a",
      "placeholder": "​",
      "style": "IPY_MODEL_36fe162e32c7464f8e32c58d2af2b452",
      "value": " 215/215 [00:00&lt;00:00, 20.8kB/s]"
     }
    },
    "ec5976c314e041a391dd771f456ec1da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed1bae75803a4f28bc912c5b0d9f9cca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f002913927f84529a06050c635b91867": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f5bc68aaef0e4a50802e37a35f7f09ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f8a0fccd8acc46dab0a2929fb4848591": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "fa4293c74ab14fdaad192c5eea6e305a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1100dcfc56e41baa634cd50a69ece62",
      "placeholder": "​",
      "style": "IPY_MODEL_62109e980a3b4f708c710669d897adcb",
      "value": "preprocessor_config.json: 100%"
     }
    },
    "fe682990628048649d28a58eeb05f5ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
